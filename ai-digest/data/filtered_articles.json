{
  "date": "2026-01-07T07:51:53.664798",
  "total_scored": 159,
  "selected_count": 15,
  "category_distribution": {
    "Production Use Cases": 5,
    "Frameworks & Tools": 10
  },
  "articles": [
    {
      "title": "Learning Human-Like RL Agents Through Trajectory Optimization With Action Quantization",
      "url": "https://arxiv.org/abs/2511.15055",
      "published": "2025-11-21T10:00:00+00:00",
      "date_verified": true,
      "description": "arXiv:2511.15055v1 Announce Type: new \nAbstract: Human-like agents have long been one of the goals in pursuing artificial intelligence. Although reinforcement learning (RL) has achieved superhuman performance in many domains, relatively little attention has been focused on designing human-like RL agents. As a result, many reward-driven RL agents often exhibit unnatural behaviors compared to humans, raising concerns for both interpretability and trustworthiness. To achieve human-like behavior in RL, this paper first formulates human-likeness as trajectory optimization, where the objective is to find an action sequence that closely aligns with human behavior while also maximizing rewards, and adapts the classic receding-horizon control to human-like learning as a tractable and efficient implementation. To achieve this, we introduce Macro Action Quantization (MAQ), a human-like RL framework that distills human demonstrations into macro actions via Vector-Quantized VAE. Experiments on D4RL Adroit benchmarks show that MAQ significantly improves human-likeness, increasing trajectory similarity scores, and achieving the highest human-likeness rankings among all RL agents in the human evaluation study. Our results also demonstrate that MAQ can be easily integrated into various off-the-shelf RL algorithms, opening a promising direction for learning human-like RL agents. Our code is available at https://rlg.iis.sinica.edu.tw/papers/MAQ.",
      "source": "cs.AI updates on arXiv.org",
      "source_url": "https://export.arxiv.org/rss/cs.AI",
      "author": "Jian-Ting Guo, Yu-Cheng Chen, Ping-Chun Hsieh, Kuo-Hao Ho, Po-Wei Huang, Ti-Rong Wu, I-Chen Wu",
      "tags": [
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "relevance_score": 80,
      "category": "Production Use Cases"
    },
    {
      "title": "Beyond GeneGPT: A Multi-Agent Architecture with Open-Source LLMs for Enhanced Genomic Question Answering",
      "url": "https://arxiv.org/abs/2511.15061",
      "published": "2025-11-21T10:00:00+00:00",
      "date_verified": true,
      "description": "arXiv:2511.15061v1 Announce Type: new \nAbstract: Genomic question answering often requires complex reasoning and integration across diverse biomedical sources. GeneGPT addressed this challenge by combining domain-specific APIs with OpenAI's code-davinci-002 large language model to enable natural language interaction with genomic databases. However, its reliance on a proprietary model limits scalability, increases operational costs, and raises concerns about data privacy and generalization.\n  In this work, we revisit and reproduce GeneGPT in a pilot study using open source models, including Llama 3.1, Qwen2.5, and Qwen2.5 Coder, within a monolithic architecture; this allows us to identify the limitations of this approach. Building on this foundation, we then develop OpenBioLLM, a modular multi-agent framework that extends GeneGPT by introducing agent specialization for tool routing, query generation, and response validation. This enables coordinated reasoning and role-based task execution.\n  OpenBioLLM matches or outperforms GeneGPT on over 90% of the benchmark tasks, achieving average scores of 0.849 on Gene-Turing and 0.830 on GeneHop, while using smaller open-source models without additional fine-tuning or tool-specific pretraining. OpenBioLLM's modular multi-agent design reduces latency by 40-50% across benchmark tasks, significantly improving efficiency without compromising model capability. The results of our comprehensive evaluation highlight the potential of open-source multi-agent systems for genomic question answering. Code and resources are available at https://github.com/ielab/OpenBioLLM.",
      "source": "cs.AI updates on arXiv.org",
      "source_url": "https://export.arxiv.org/rss/cs.AI",
      "author": "Haodong Chen, Guido Zuccon, Teerapong Leelanupab",
      "tags": [
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "relevance_score": 80,
      "category": "Frameworks & Tools"
    },
    {
      "title": "Knowledge-Informed Automatic Feature Extraction via Collaborative Large Language Model Agents",
      "url": "https://arxiv.org/abs/2511.15074",
      "published": "2025-11-21T10:00:00+00:00",
      "date_verified": true,
      "description": "arXiv:2511.15074v1 Announce Type: new \nAbstract: The performance of machine learning models on tabular data is critically dependent on high-quality feature engineering. While Large Language Models (LLMs) have shown promise in automating feature extraction (AutoFE), existing methods are often limited by monolithic LLM architectures, simplistic quantitative feedback, and a failure to systematically integrate external domain knowledge. This paper introduces Rogue One, a novel, LLM-based multi-agent framework for knowledge-informed automatic feature extraction. Rogue One operationalizes a decentralized system of three specialized agents-Scientist, Extractor, and Tester-that collaborate iteratively to discover, generate, and validate predictive features. Crucially, the framework moves beyond primitive accuracy scores by introducing a rich, qualitative feedback mechanism and a \"flooding-pruning\" strategy, allowing it to dynamically balance feature exploration and exploitation. By actively incorporating external knowledge via an integrated retrieval-augmented (RAG) system, Rogue One generates features that are not only statistically powerful but also semantically meaningful and interpretable. We demonstrate that Rogue One significantly outperforms state-of-the-art methods on a comprehensive suite of 19 classification and 9 regression datasets. Furthermore, we show qualitatively that the system surfaces novel, testable hypotheses, such as identifying a new potential biomarker in the myocardial dataset, underscoring its utility as a tool for scientific discovery.",
      "source": "cs.AI updates on arXiv.org",
      "source_url": "https://export.arxiv.org/rss/cs.AI",
      "author": "Henrik Bradland, Morten Goodwin, Vladimir I. Zadorozhny, Per-Arne Andersen",
      "tags": [
        "cs.AI",
        "cs.CL"
      ],
      "relevance_score": 80,
      "category": "Frameworks & Tools"
    },
    {
      "title": "How agents can use filesystems for context engineering",
      "url": "https://blog.langchain.com/how-agents-can-use-filesystems-for-context-engineering/",
      "published": "2025-11-21T23:45:13+00:00",
      "date_verified": true,
      "description": "<p>A key feature of <a href=\"https://blog.langchain.com/deep-agents/\">deep agents</a> is their access to a set of filesystem tools. Deep agents can use these tools to read, write, edit, list, and search for files in their filesystem.</p><p>In this post, we&#x2019;ll walk through why we think filesystems are important for agents. In</p>",
      "source": "LangChain Blog",
      "source_url": "https://blog.langchain.dev/rss/",
      "author": "LangChain Accounts",
      "tags": [],
      "relevance_score": 75,
      "category": "Frameworks & Tools"
    },
    {
      "title": "Grok 4.1 Fast's compelling dev access and Agent Tools API overshadowed by Musk glazing",
      "url": "https://venturebeat.com/ai/grok-4-1-fasts-compelling-dev-access-and-agent-tools-api-overshadowed-by",
      "published": "2025-11-21T04:57:00+00:00",
      "date_verified": true,
      "description": "<p>Elon Musk&#x27;s frontier generative AI startup xAI<a href=\"https://x.ai/news/grok-4-1-fast\"> formally opened developer access to its Grok 4.1 Fast models</a> last night and introduced a new Agent Tools API—but the technical milestones were immediately subverted by a wave of public ridicule about Grok&#x27;s responses on the social network X over the last few days praising its creator Musk as <a href=\"https://x.com/agraybee/status/1991578106239545455?s=20\">more athletic than championship-winning American football players</a> and legendary <a href=\"https://x.com/pitdesi/status/1991532840811655418?s=20\">boxer Mike Tyson</a>, despite having displayed no public prowess at either sport.</p><p>They emerge as yet another black eye for xAI&#x27;s Grok following the <a href=\"https://www.marketingaiinstitute.com/blog/grok-model-update\">&quot;MechaHitler&quot; scandal in the summer of 2025</a>, in which an earlier version of Grok adopted a verbally antisemitic persona inspired by the late German dictator and Holocaust architect, and an incident in May 2025 which it replied to X users to discuss <a href=\"https://venturebeat.com/ai/elon-musks-xai-tries-to-explain-groks-south-african-race-relations-freakout-the-other-day\">unfounded claims of &quot;white genocide&quot; </a>in Musk&#x27;s home country of South Africa to unrelated subject matter.</p><p>This time, X users shared dozens of examples of Grok alleging Musk was stronger or more performant than elite athletes and a greater thinker than luminaries such as Albert Einstein, sparking questions about the AI&#x27;s reliability, bias controls, adversarial prompting defenses, and the credibility of xAI’s public claims about “maximally truth-seeking” models. .</p><p>Against this backdrop, xAI’s actual developer-focused announcement—the first-ever API availability for Grok 4.1 Fast Reasoning, Grok 4.1 Fast Non-Reasoning, and the Agent Tools API—landed in a climate dominated by memes, skepticism, and renewed scrutiny.</p><h1><b>How the Grok Musk Glazing Controversy Overshadowed the API Release</b></h1><p>Although <a href=\"https://venturebeat.com/ai/musks-xai-launches-grok-4-1-with-lower-hallucination-rate-on-the-web-and\">Grok 4.1 was announced on the evening of Monday, November 17, 2025</a> as available to consumers via the X and Grok apps and websites, the<a href=\"https://x.com/xai/status/1991284813727474073\"> API launch announced last night</a>, on November 19, was intended to mark a developer-focused expansion. </p><p>Instead, the conversation across X shifted sharply toward Grok’s behavior in consumer channels.</p><p>Between November 17–20, users discovered that Grok would frequently deliver exaggerated, implausible praise for Musk when prompted—sometimes subtly, often brazenly. </p><p>Responses declaring Musk “more fit than LeBron James,” a superior quarterback to Peyton Manning, or “smarter than Albert Einstein” gained massive engagement. </p><div></div><p>When paired with identical prompts substituting “Bill Gates” or other figures, Grok often responded far more critically, suggesting inconsistent preference handling or latent alignment drift.</p><div></div><ul><li><p><b>Screenshots spread by high-engagement accounts</b> (e.g., <a href=\"https://x.com/SilvermanJacob/status/1991565290967298522\">@SilvermanJacob</a>, @StatisticUrban) framed Grok as unreliable or compromised.</p></li><li><p><b>Memetic commentary</b>—“Elon’s only friend is Grok”—became shorthand for perceived sycophancy.</p></li><li><p><b>Media coverage</b>, including a November 20 report from <a href=\"https://www.theverge.com/ai-artificial-intelligence/825675/groks-elon-musk-worship-is-getting-weird\">The Verge,</a> characterized Grok’s responses as “weird worship,” highlighting claims that Musk is “as smart as da Vinci” and “fitter than LeBron James.”</p></li><li><p><b>Critical threads</b> argued that Grok’s design choices replicated past alignment failures, such as a July 2025 incident where Grok generated problematic praise of Adolf Hitler under certain prompting conditions.</p></li></ul><p>The viral nature of the glazing overshadowed the technical release and complicated xAI’s messaging about accuracy and trustworthiness.</p><h3><b>Implications for Developer Adoption and Trust</b></h3><p>The juxtaposition of a major API release with a public credibility crisis raises several concerns:</p><ol><li><p><b>Alignment Controls</b>\n The glazing behavior suggests that prompt adversariality may expose latent preference biases, undermining claims of “truth-maximization.”</p></li><li><p><b>Brand Contamination Across Deployment Contexts</b>\n Though the consumer chatbot and API-accessible model share lineage, developers may conflate the reliability of both—even if safeguards differ.</p></li><li><p><b>Risk in Agentic Systems</b>\n The Agent Tools API gives Grok abilities such as web search, code execution, and document retrieval. Bias-driven misjudgments in those contexts could have material consequences.</p></li><li><p><b>Regulatory Scrutiny</b>\n Biased outputs that systematically favor a CEO or public figure could attract attention from consumer protection regulators evaluating AI representational neutrality.</p></li><li><p><b>Developer Hesitancy</b>\n Early adopters may wait for evidence that the model version exposed through the API is not subject to the same glazing behaviors seen in consumer channels.</p></li></ol><p><a href=\"https://x.com/elonmusk/status/1991624623407161383?s=20\">Musk himself attempted to defuse the situation</a> with a self-deprecating X post this evening, writing:</p><blockquote><p>“Grok was unfortunately manipulated by adversarial prompting into saying absurdly positive things about me. For the record, I am a fat retard.”</p></blockquote><p>While intended to signal transparency, the admission did not directly address whether the root cause was adversarial prompting alone or whether model training introduced unintentional positive priors. </p><p>Nor did it clarify whether the API-exposed versions of Grok 4.1 Fast differ meaningfully from the consumer version that produced the offending outputs.</p><p>Until xAI provides deeper technical detail about prompt vulnerabilities, preference modeling, and safety guardrails, the controversy is likely to persist.</p><h2><b>Two Grok 4.1 Models Available on xAI API</b></h2><p>Although consumers using Grok apps gained access to Grok 4.1 Fast earlier in the week, developers could not previously use the model through the xAI API. The latest release closes that gap by adding two new models to the public model catalog:</p><ul><li><p><b>grok-4-1-fast-reasoning</b> — designed for maximal reasoning performance and complex tool workflows</p></li><li><p><b>grok-4-1-fast-non-reasoning</b> — optimized for extremely fast responses</p></li></ul><p>Both models support a 2 million–token context window, aligning them with xAI’s long-context roadmap and providing substantial headroom for multistep agent tasks, document processing, and research workflows.</p><p>The new additions appear alongside updated entries in xAI’s pricing and rate-limit tables, confirming that they now function as first-class API endpoints across xAI infrastructure and routing partners such as OpenRouter.</p><h2><b>Agent Tools API: A New Server-Side Tool Layer</b></h2><p>The other major component of the announcement is the <b>Agent Tools API</b>, which introduces a unified mechanism for Grok to call tools across a range of capabilities:</p><ul><li><p><b>Search Tools</b> including a direct link to <b>X (Twitter) search</b> for real-time conversations and <b>web search</b> for broad external retrieval.</p></li><li><p><b>Files Search: </b>Retrieval and citation of relevant documents uploaded by users</p></li><li><p><b>Code Execution: </b>A secure Python sandbox for analysis, simulation, and data processing</p></li><li><p><b>MCP (Model Context Protocol) Integration: </b>Connects Grok agents with third-party tools or custom enterprise systems</p></li></ul><p>xAI emphasizes that the API handles all infrastructure complexity—including sandboxing, key management, rate limiting, and environment orchestration—on the server side. Developers simply declare which tools are available, and Grok autonomously decides when and how to invoke them. The company highlights that the model frequently performs multi-tool, multi-turn workflows in parallel, reducing latency for complex tasks.</p><h2><b>How the New API Layer Leverages Grok 4.1 Fast</b></h2><p>While the model existed before today’s API release, Grok 4.1 Fast was trained explicitly for tool-calling performance. The model’s long-horizon reinforcement learning tuning supports autonomous planning, which is essential for agent systems that chain multiple operations.</p><p>Key behaviors highlighted by xAI include:</p><ul><li><p><b>Consistent output quality across the full 2M token context window</b>, enabled by long-horizon RL</p></li><li><p><b>Reduced hallucination rate</b>, cut in half compared with Grok 4 Fast while maintaining Grok 4’s factual accuracy performance</p></li><li><p><b>Parallel tool use</b>, where Grok executes multiple tool calls concurrently when solving multi-step problems</p></li><li><p><b>Adaptive reasoning</b>, allowing the model to plan tool sequences over several turns</p></li></ul><p>This behavior aligns directly with the Agent Tools API’s purpose: to give Grok the external capabilities necessary for autonomous agent work.</p><h2><b>Benchmark Results Demonstrating Highest Agentic Performance</b></h2><p>xAI released a set of benchmark results intended to illustrate how Grok 4.1 Fast performs when paired with the Agent Tools API, emphasizing scenarios that rely on tool calling, long-context reasoning, and multi-step task execution. </p><p>On <b>τ²-bench Telecom</b>, a benchmark built to replicate real-world customer-support workflows involving tool use, Grok 4.1 Fast achieved the highest score among all listed models — outpacing even Google&#x27;s new Gemini 3 Pro and OpenAI&#x27;s recent 5.1 on high reasoning — while also achieving among the lowest prices for developers and users. The evaluation, independently verified by Artificial Analysis, cost $105 to complete and served as one of xAI’s central claims of superiority in agentic performance.</p><p>In structured function-calling tests, Grok 4.1 Fast Reasoning recorded a 72 percent overall accuracy on the Berkeley Function Calling v4 benchmark, a result accompanied by a reported cost of $400 for the run. </p><p>xAI noted that Gemini 3 Pro’s comparative result in this benchmark stemmed from independent estimates rather than an official submission, leaving some uncertainty in cross-model comparisons.</p><p>Long-horizon evaluations further underscored the model’s design emphasis on stability across large contexts. In multi-turn tests involving extended dialog and expanded context windows, Grok 4.1 Fast outperformed both Grok 4 Fast and the earlier Grok 4, aligning with xAI’s claims that long-horizon reinforcement learning helped mitigate the typical degradation seen in models operating at the two-million-token scale.</p><p>A second cluster of benchmarks—Research-Eval, FRAMES, and X Browse—highlighted Grok 4.1 Fast’s capabilities in tool-augmented research tasks. </p><p>Across all three evaluations, Grok 4.1 Fast paired with the Agent Tools API earned the highest scores among the models with published results. It also delivered the lowest average cost per query in Research-Eval and FRAMES, reinforcing xAI’s messaging on cost-efficient research performance. </p><p>In X Browse, an internal xAI benchmark assessing multihop search capabilities across the X platform, Grok 4.1 Fast again led its peers, though Gemini 3 Pro lacked cost data for direct comparison.</p><h2><b>Developer Pricing and Temporary Free Access</b></h2><p>API pricing for Grok 4.1 Fast is as follows:</p><ul><li><p><b>Input tokens:</b> $0.20 per 1M</p></li><li><p><b>Cached input tokens:</b> $0.05 per 1M</p></li><li><p><b>Output tokens:</b> $0.50 per 1M</p></li><li><p><b>Tool calls:</b> From $5 per 1,000 successful tool invocations</p></li></ul><p>To facilitate early experimentation:</p><ul><li><p><b>Grok 4.1 Fast is free on OpenRouter until December 3rd.</b></p></li><li><p><b>The Agent Tools API is also free through December 3rd via the xAI API.</b></p></li></ul><p>When paying for the models outside of the free period, Grok 4.1 Fast reasoning and non-reasoning are both among the cheaper options from major frontier labs through their own APIs. See below:</p><table><tbody><tr><td><p><b>Model</b></p></td><td><p><b>Input (/1M)</b></p></td><td><p><b>Output (/1M)</b></p></td><td><p><b>Total Cost</b></p></td><td><p><b>Source</b></p></td></tr><tr><td><p>Qwen 3 Turbo</p></td><td><p>$0.05</p></td><td><p>$0.20</p></td><td><p>$0.25</p></td><td><p><a href=\"https://www.alibabacloud.com/en/campaign/qwen-ai-landing-page?_p_lc=1&amp;src=qwenai\">Alibaba Cloud</a></p></td></tr><tr><td><p>ERNIE 4.5 Turbo</p></td><td><p>$0.11</p></td><td><p>$0.45</p></td><td><p>$0.56</p></td><td><p><a href=\"https://cloud.baidu.com/doc/WENXINWORKSHOP/s/Blfmc9do4\">Qianfan</a></p></td></tr><tr><td><p><b>Grok 4.1 Fast (reasoning)</b></p></td><td><p><b>$0.20</b></p></td><td><p><b>$0.50</b></p></td><td><p><b>$0.70</b></p></td><td><p><a href=\"https://docs.x.ai/docs/models?cluster=us-east-1#detailed-pricing-for-all-grok-models\">xAI</a></p></td></tr><tr><td><p><b>Grok 4.1 Fast (non-reasoning)</b></p></td><td><p><b>$0.20</b></p></td><td><p><b>$0.50</b></p></td><td><p><b>$0.70</b></p></td><td><p><a href=\"https://docs.x.ai/docs/models?cluster=us-east-1#detailed-pricing-for-all-grok-models\">xAI</a></p></td></tr><tr><td><p>deepseek-chat (V3.2-Exp)</p></td><td><p>$0.28</p></td><td><p>$0.42</p></td><td><p>$0.70</p></td><td><p><a href=\"https://api-docs.deepseek.com/quick_start/pricing\">DeepSeek</a></p></td></tr><tr><td><p>deepseek-reasoner (V3.2-Exp)</p></td><td><p>$0.28</p></td><td><p>$0.42</p></td><td><p>$0.70</p></td><td><p><a href=\"https://api-docs.deepseek.com/quick_start/pricing\">DeepSeek</a></p></td></tr><tr><td><p>Qwen 3 Plus</p></td><td><p>$0.40</p></td><td><p>$1.20</p></td><td><p>$1.60</p></td><td><p><a href=\"https://www.alibabacloud.com/en/campaign/qwen-ai-landing-page?_p_lc=1&amp;src=qwenai\">Alibaba Cloud</a></p></td></tr><tr><td><p>ERNIE 5.0</p></td><td><p>$0.85</p></td><td><p>$3.40</p></td><td><p>$4.25</p></td><td><p><a href=\"https://cloud.baidu.com/doc/WENXINWORKSHOP/s/Blfmc9do4\">Qianfan</a></p></td></tr><tr><td><p>Qwen-Max</p></td><td><p>$1.60</p></td><td><p>$6.40</p></td><td><p>$8.00</p></td><td><p><a href=\"https://www.alibabacloud.com/en/campaign/qwen-ai-landing-page?_p_lc=1&amp;src=qwenai\">Alibaba Cloud</a></p></td></tr><tr><td><p>GPT-5.1</p></td><td><p>$1.25</p></td><td><p>$10.00</p></td><td><p>$11.25</p></td><td><p><a href=\"https://openai.com/pricing\">OpenAI</a></p></td></tr><tr><td><p>Gemini 2.5 Pro (≤200K)</p></td><td><p>$1.25</p></td><td><p>$10.00</p></td><td><p>$11.25</p></td><td><p><a href=\"https://ai.google.dev/gemini-api/docs/pricing\">Google</a></p></td></tr><tr><td><p>Gemini 3 Pro (≤200K)</p></td><td><p>$2.00</p></td><td><p>$12.00</p></td><td><p>$14.00</p></td><td><p><a href=\"https://ai.google.dev/gemini-api/docs/pricing\">Google</a></p></td></tr><tr><td><p>Gemini 2.5 Pro (&gt;200K)</p></td><td><p>$2.50</p></td><td><p>$15.00</p></td><td><p>$17.50</p></td><td><p><a href=\"https://ai.google.dev/gemini-api/docs/pricing\">Google</a></p></td></tr><tr><td><p><b>Grok 4 (0709)</b></p></td><td><p><b>$3.00</b></p></td><td><p><b>$15.00</b></p></td><td><p><b>$18.00</b></p></td><td><p><a href=\"https://docs.x.ai/docs/models?cluster=us-east-1#detailed-pricing-for-all-grok-models\">xAI</a></p></td></tr><tr><td><p>Gemini 3 Pro (&gt;200K)</p></td><td><p>$4.00</p></td><td><p>$18.00</p></td><td><p>$22.00</p></td><td><p><a href=\"https://ai.google.dev/gemini-api/docs/pricing\">Google</a></p></td></tr><tr><td><p>Claude Opus 4.1</p></td><td><p>$15.00</p></td><td><p>$75.00</p></td><td><p>$90.00</p></td><td><p><a href=\"https://docs.anthropic.com/claude/docs/models-overview\">Anthropic</a></p></td></tr></tbody></table><h2><b>How Enterprises Should Evaluate Grok 4.1 Fast in Light of Performance, Cost, and Trust</b></h2><p>For enterprises evaluating frontier-model deployments, Grok 4.1 Fast presents a compelling combination of high performance and low operational cost. Across multiple agentic and function-calling benchmarks, the model consistently outperforms or matches leading systems like Gemini 3 Pro, GPT-5.1 (high), and Claude 4.5 Sonnet, while operating inside a far more economical cost envelope. </p><p>At $0.70 per million tokens, both Grok 4.1 Fast variants sit only marginally above ultracheap models like Qwen 3 Turbo but deliver accuracy levels in line with systems that cost 10–20× more per unit. The τ²-bench Telecom results reinforce this value proposition: Grok 4.1 Fast not only achieved the highest score in its test cohort but also appears to be the lowest-cost model in that benchmark run. In practical terms, this gives enterprises an unusually favorable cost-to-intelligence ratio, particularly for workloads involving multistep planning, tool use, and long-context reasoning.</p><p>However, performance and pricing are only part of the equation for organizations considering large-scale adoption. The recent “glazing” controversy from Grok’s consumer deployment on X — combined with the earlier &quot;MechaHitler&quot; and &quot;White Genocid&quot; incidents — expose credibility and trust-surface risks that enterprises cannot ignore. </p><p>Even if the API models are technically distinct from the consumer-facing variant, the inability to prevent sycophantic, adversarially-induced bias in a high-visibility environment raises legitimate concerns about downstream reliability in operational contexts. Enterprise procurement teams will rightly ask whether similar vulnerabilities—preference skew, alignment drift, or context-sensitive bias—could surface when Grok is connected to production databases, workflow engines, code-execution tools, or research pipelines.</p><p>The introduction of the Agent Tools API raises the stakes further. Grok 4.1 Fast is not just a text generator—it is now an orchestrator of web searches, X-data queries, document retrieval operations, and remote Python execution. These agentic capabilities amplify productivity but also expand the blast radius of any misalignment. A model that can over-index on flattering a public figure could, in principle, also misprioritize results, mis-handle safety boundaries, or deliver skewed interpretations when operating with real-world data. </p><p>Enterprises therefore need a clear understanding of how xAI isolates, audits, and hardens its API models relative to the consumer-facing Grok whose failures drove the latest scrutiny.</p><p>The result is a mixed strategic picture. On performance and price, Grok 4.1 Fast is highly competitive—arguably one of the strongest value propositions in the modern LLM market. </p><p>But xAI’s enterprise appeal will ultimately depend on whether the company can convincingly demonstrate that the alignment instability, susceptibility to adversarial prompting, and bias-amplifying behavior observed on X do not translate into its developer-facing platform. </p><p>Without transparent safeguards, auditability, and reproducible evaluation across the very tools that enable autonomous operation, organizations may hesitate to commit core workloads to a system whose reliability is still the subject of public doubt. </p><p>For now, Grok 4.1 Fast is a technically impressive and economically efficient option—one that enterprises should test, benchmark, and validate rigorously before allowing it to take on mission-critical tas</p>",
      "source": "AI | VentureBeat",
      "source_url": "https://venturebeat.com/category/ai/feed/",
      "author": "carl.franzen@venturebeat.com (Carl Franzen)",
      "tags": [
        "AI"
      ],
      "relevance_score": 75,
      "category": "Production Use Cases"
    },
    {
      "title": "1.23.0-beta.0",
      "url": "https://github.com/modelcontextprotocol/typescript-sdk/releases/tag/1.23.0-beta.0",
      "published": "2025-11-21T03:01:34+00:00",
      "date_verified": true,
      "description": "<h2>Special Note: zod v4</h2>\n<p>This <code>beta</code> release adds support for zod v4, a highly requested addition.</p>\n<p>Special thanks to <a class=\"user-mention notranslate\" href=\"https://github.com/dclark27\">@dclark27</a> <a class=\"user-mention notranslate\" href=\"https://github.com/colinhacks\">@colinhacks</a> for all the work on <a class=\"issue-link js-issue-link\" href=\"https://github.com/modelcontextprotocol/typescript-sdk/pull/1040\">#1040</a> which adds this support while staying backwards compatible with zod v3!</p>\n<p>NOTE: if you run into any issues with zod v4 in this SDK please raise an issue! We'll be monitoring actively for any issues while trying to use zod v4 as it's a highly anticipated update. In order to make this upgrade possible we needed to also require v3.25+ for zod v3. You may need to update your version of zod v3 if you've been using an older one. We're also making this a <code>beta</code> release while we get initial feedback on the update here.</p>\n<h2>What's Changed</h2>\n<ul>\n<li>Migrate to vitest from jest by <a class=\"user-mention notranslate\" href=\"https://github.com/mattzcarey\">@mattzcarey</a> in <a class=\"issue-link js-issue-link\" href=\"https://github.com/modelcontextprotocol/typescript-sdk/pull/1074\">#1074</a></li>\n<li>ci: add workflow_dispatch trigger for manual CI runs by <a class=\"user-mention notranslate\" href=\"https://github.com/felixweinberger\">@felixweinberger</a> in <a class=\"issue-link js-issue-link\" href=\"https://github.com/modelcontextprotocol/typescript-sdk/pull/1114\">#1114</a></li>\n<li>Fix: @types/node incompatibility with vite warnings on npm install by <a class=\"user-mention notranslate\" href=\"https://github.com/KKonstantinov\">@KKonstantinov</a> in <a class=\"issue-link js-issue-link\" href=\"https://github.com/modelcontextprotocol/typescript-sdk/pull/1121\">#1121</a></li>\n<li>Fix: clean up accidental spec.types.ts by <a class=\"user-mention notranslate\" href=\"https://github.com/KKonstantinov\">@KKonstantinov</a> in <a class=\"issue-link js-issue-link\" href=\"https://github.com/modelcontextprotocol/typescript-sdk/pull/1122\">#1122</a></li>\n<li>fix: Pass RequestInit options to auth requests by <a class=\"user-mention notranslate\" href=\"https://github.com/dsp-ant\">@dsp-ant</a> in <a class=\"issue-link js-issue-link\" href=\"https://github.com/modelcontextprotocol/typescript-sdk/pull/1066\">#1066</a></li>\n<li>SEP-1036: URL Elicitation by <a class=\"user-mention notranslate\" href=\"https://github.com/nbarbettini\">@nbarbettini</a> in <a class=\"issue-link js-issue-link\" href=\"https://github.com/modelcontextprotocol/typescript-sdk/pull/1105\">#1105</a></li>\n<li>add none to test and the router. by <a class=\"user-mention notranslate\" href=\"https://github.com/m-henderson\">@m-henderson</a> in <a class=\"issue-link js-issue-link\" href=\"https://github.com/modelcontextprotocol/typescript-sdk/pull/1116\">#1116</a></li>\n<li>[auth] Adjust scope management to line up with SEP-835 by <a class=\"user-mention notranslate\" href=\"https://github.com/pcarleton\">@pcarleton</a> in <a class=\"issue-link js-issue-link\" href=\"https://github.com/modelcontextprotocol/typescript-sdk/pull/1133\">#1133</a></li>\n<li>chore: add .idea/ to .gitignore by <a class=\"user-mention notranslate\" href=\"https://github.com/maxisbey\">@maxisbey</a> in <a class=\"issue-link js-issue-link\" href=\"https://github.com/modelcontextprotocol/typescript-sdk/pull/1134\">#1134</a></li>\n<li>chore: remove unused @types/eslint__js dependency by <a class=\"user-mention notranslate\" href=\"https://github.com/mattzcarey\">@mattzcarey</a> in <a class=\"issue-link js-issue-link\" href=\"https://github.com/modelcontextprotocol/typescript-sdk/pull/1128\">#1128</a></li>\n<li>feat: url based client metadata registration (SEP 991) by <a class=\"user-mention notranslate\" href=\"https://github.com/mattzcarey\">@mattzcarey</a> in <a class=\"issue-link js-issue-link\" href=\"https://github.com/modelcontextprotocol/typescript-sdk/pull/1127\">#1127</a></li>\n<li>feat: zod v4 with backwards compatibility for v3.25+ by <a class=\"user-mention notranslate\" href=\"https://github.com/dclark27\">@dclark27</a> in <a class=\"issue-link js-issue-link\" href=\"https://github.com/modelcontextprotocol/typescript-sdk/pull/1040\">#1040</a></li>\n<li>fix: remove pnpm lock and regenerate package-lock by <a class=\"user-mention notranslate\" href=\"https://github.com/mattzcarey\">@mattzcarey</a> in <a class=\"issue-link js-issue-link\" href=\"https://github.com/modelcontextprotocol/typescript-sdk/pull/1138\">#1138</a></li>\n<li>docs: update installation instructions for zod peer dependency by <a class=\"user-mention notranslate\" href=\"https://github.com/mattzcarey\">@mattzcarey</a> in <a class=\"issue-link js-issue-link\" href=\"https://github.com/modelcontextprotocol/typescript-sdk/pull/1139\">#1139</a></li>\n<li>Implement SEP-1577 - Sampling With Tools by <a class=\"user-mention notranslate\" href=\"https://github.com/ochafik\">@ochafik</a> in <a class=\"issue-link js-issue-link\" href=\"https://github.com/modelcontextprotocol/typescript-sdk/pull/1101\">#1101</a></li>\n<li>Follow up: unify v3 and v4 zod types via describe matrix and a test helper by <a class=\"user-mention notranslate\" href=\"https://github.com/KKonstantinov\">@KKonstantinov</a> in <a class=\"issue-link js-issue-link\" href=\"https://github.com/modelcontextprotocol/typescript-sdk/pull/1141\">#1141</a></li>\n<li>chore: Add deprecated marker to old elicitInput overload by <a class=\"user-mention notranslate\" href=\"https://github.com/nbarbettini\">@nbarbettini</a> in <a class=\"issue-link js-issue-link\" href=\"https://github.com/modelcontextprotocol/typescript-sdk/pull/1142\">#1142</a></li>\n<li>fix: Connect error in URL elicitation example by <a class=\"user-mention notranslate\" href=\"https://github.com/nbarbettini\">@nbarbettini</a> in <a class=\"issue-link js-issue-link\" href=\"https://github.com/modelcontextprotocol/typescript-sdk/pull/1136\">#1136</a></li>\n<li>Support upscoping on insufficient_scope 403 by <a class=\"user-mention notranslate\" href=\"https://github.com/Nayana-Parameswarappa\">@Nayana-Parameswarappa</a> in <a class=\"issue-link js-issue-link\" href=\"https://github.com/modelcontextprotocol/typescript-sdk/pull/1115\">#1115</a></li>\n<li>Support beta releases by publishing with --tag beta by <a class=\"user-mention notranslate\" href=\"https://github.com/felixweinberger\">@felixweinberger</a> in <a class=\"issue-link js-issue-link\" href=\"https://github.com/modelcontextprotocol/typescript-sdk/pull/1146\">#1146</a></li>\n<li>Bump version to 1.23.0-beta.0 by <a class=\"user-mention notranslate\" href=\"https://github.com/felixweinberger\">@felixweinberger</a> in <a class=\"issue-link js-issue-link\" href=\"https://github.com/modelcontextprotocol/typescript-sdk/pull/1147\">#1147</a></li>\n</ul>\n<h2>New Contributors</h2>\n<ul>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/nbarbettini\">@nbarbettini</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/modelcontextprotocol/typescript-sdk/pull/1105\">#1105</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/m-henderson\">@m-henderson</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/modelcontextprotocol/typescript-sdk/pull/1116\">#1116</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/maxisbey\">@maxisbey</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/modelcontextprotocol/typescript-sdk/pull/1134\">#1134</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/dclark27\">@dclark27</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/modelcontextprotocol/typescript-sdk/pull/1040\">#1040</a></li>\n<li><a class=\"user-mention notranslate\" href=\"https://github.com/Nayana-Parameswarappa\">@Nayana-Parameswarappa</a> made their first contribution in <a class=\"issue-link js-issue-link\" href=\"https://github.com/modelcontextprotocol/typescript-sdk/pull/1115\">#1115</a></li>\n</ul>\n<p><strong>Full Changelog</strong>: <a class=\"commit-link\" href=\"https://github.com/modelcontextprotocol/typescript-sdk/compare/1.22.0...1.23.0-beta.0\"><tt>1.22.0...1.23.0-beta.0</tt></a></p>",
      "source": "Release notes from typescript-sdk",
      "source_url": "https://github.com/modelcontextprotocol/typescript-sdk/releases.atom",
      "author": "felixweinberger",
      "tags": [],
      "relevance_score": 75,
      "category": "Frameworks & Tools"
    },
    {
      "title": "Google's upgraded Nano Banana Pro AI image model hailed as 'absolutely bonkers' for enterprises and users",
      "url": "https://venturebeat.com/ai/googles-upgraded-nano-banana-pro-ai-image-model-hailed-as-absolutely-bonkers",
      "published": "2025-11-21T01:20:00+00:00",
      "date_verified": true,
      "description": "<p>Infographics rendered without a single spelling error. Complex diagrams one-shotted from paragraph prompts. Logos restored from fragments. And visual outputs so sharp with so much text density and accuracy, one developer simply called it “absolutely bonkers.”</p><p>Google DeepMind’s <a href=\"https://blog.google/technology/ai/nano-banana-pro/\">newly released Nano Banana Pro</a>—officially Gemini 3 Pro Image—has drawn astonishment from both the developer community and enterprise AI engineers. </p><p>But behind the viral praise lies something more transformative: a model built not just to impress, but to integrate deeply across Google’s AI stack—from Gemini API and Vertex AI to Workspace apps, Ads, and Google AI Studio.</p><p>Unlike earlier image models, which targeted casual users or artistic use cases, Gemini 3 Pro Image introduces studio-quality, multimodal image generation for structured workflows—with high resolution, multilingual accuracy, layout consistency, and real-time knowledge grounding. It’s engineered for technical buyers, orchestration teams, and enterprise-scale automation, not just creative exploration.</p><p>Benchmarks already show the model outperforming peers in overall visual quality, infographic generation, and text rendering accuracy. And as real-world users push it to its limits—from medical illustrations to AI memes—the model is revealing itself as both a new creative tool and a visual reasoning system for the enterprise stack.</p><h2><b>Built for Structured Multimodal Reasoning</b></h2><p>Gemini 3 Pro Image isn’t just drawing pretty pictures—it’s leveraging the reasoning layer of Gemini 3 Pro to generate visuals that communicate structure, intent, and factual grounding. </p><p>The model is capable of generating UX flows, educational diagrams, storyboards, and mockups from language prompts, and can incorporate up to 14 source images with consistent identity and layout fidelity across subjects.</p><p>Google describes the model as “a higher-fidelity model built on Gemini 3 Pro for developers to access studio-quality image generation,” and confirms it is now available via Gemini API, Google AI Studio, and Vertex AI for enterprise access.</p><p>In Antigravity, Google’s new AI vibe coding platform built by the former Windsurf co-founders it hired earlier this year, Gemini 3 Pro Image is already being used to create dynamic UI prototypes with image assets rendered before code is written. The same capabilities are rolling out to Google’s enterprise-facing products like Workspace Vids, Slides, and Google Ads, giving teams precise control over asset layout, lighting, typography, and image composition.</p><h2><b>High-Resolution Output, Localization, and Real-Time Grounding</b></h2><p>The model supports output resolutions of up to 2K and 4K, and includes studio-level controls over camera angle, color grading, focus, and lighting. It handles multilingual prompts, semantic localization, and in-image text translation, enabling workflows like:</p><ul><li><p>Translating packaging or signage while preserving layout</p></li><li><p>Updating UX mockups for regional markets</p></li><li><p>Generating consistent ad variants with product names and pricing changed by locale</p></li></ul><p>One of the clearest use cases is infographics—both technical and commercial. </p><p>Dr. Derya Unutmaz, an immunologist, generated a full medical illustration describing the stages of CAR-T cell therapy from lab to patient, praising the result as “perfect.” AI educator Dan Mac created a visual guide explaining transformer models “for a non-technical person” and called the result “unbelievable.”</p><p>Even complex structured visuals like full restaurant menus, chalkboard lecture visuals, or multi-character comic strips have been shared online—generated in a single prompt, with coherent typography, layout, and subject continuity.</p><h2><b>Benchmarks Signal a Lead in Compositional Image Generation</b></h2><p>Independent GenAI-Bench results show Gemini 3 Pro Image as a state-of-the-art performer across key categories:</p><ul><li><p>It ranks highest in <b>overall user preference</b>, suggesting strong visual coherence and prompt alignment.</p></li><li><p>It leads in <b>visual quality</b>, ahead of competitors like GPT-Image 1 and Seedream v4.</p></li><li><p>Most notably, it dominates in <b>infographic generation</b>, outscoring even Google’s own previous model, Gemini 2.5 Flash.</p></li></ul><p>Additional benchmarks released by Google show Gemini 3 Pro Image with lower text error rates across multiple languages, as well as stronger performance in image editing fidelity.</p><p>The difference becomes especially apparent in structured reasoning tasks. Where previous models might approximate style or fill in layout gaps, Gemini 3 Pro Image demonstrates consistency across panels, accurate spatial relationships, and context-aware detail preservation—crucial for systems generating diagrams, documentation, or training visuals at scale.</p><h2><b>Pricing Is Competitive for the Quality</b></h2><p>For developers and enterprise teams accessing Gemini 3 Pro Image via the Gemini API or Google AI Studio, pricing is tiered by resolution and usage. </p><p>Input tokens for images are priced at $0.0011 per image (equivalent to 560 tokens or $0.067 per image), while output pricing depends on resolution: standard 1K and 2K images cost approximately $0.134 each (1,120 tokens), and high-resolution 4K images cost $0.24 (2,000 tokens). </p><p>Text input and output are priced in line with Gemini 3 Pro: $2.00 per million input tokens and $12.00 per million output tokens when using the model’s reasoning capabilities. </p><p>The free tier currently does not include access to Nano Banana Pro, and unlike free-tier models, the paid-tier generations are not used to train Google’s systems.</p><p>Here’s a comparison table of major image-generation APIs for developers/enterprises, followed by a discussion of how they stack up (including the tiered pricing for Gemini 3 Pro Image / “Nano Banana Pro”).</p><table><tbody><tr><td><p><b>Model / Service</b></p></td><td><p><b>Approximate Price per Image or Token-Unit</b></p></td><td><p><b>Key Notes / Resolution Tiers</b></p></td></tr><tr><td><p>Google – Gemini 3 Pro Image (Nano Banana Pro)</p></td><td><p>Input (image): ~$0.067 per image (560 tokens). Output: ~$0.134 per image for 1K/2K (1120 tokens), ~$0.24 per image for 4K (2000 tokens). Text: $2.00 per million input tokens &amp; $12.00 per million output tokens (≤200k token context) </p></td><td><p>Tiered by resolution; paid-tier images are <i>not</i> used to train Google’s systems.</p></td></tr><tr><td><p>OpenAI – DALL-E 3 API</p></td><td><p>~ $0.04/image for 1024×1024 standard; ~$0.08/image for larger/resolution/HD. </p></td><td><p>Lower cost per image; resolution and quality tiers adjust pricing.</p></td></tr><tr><td><p>OpenAI – GPT-Image-1 (via Azure/OpenAI)</p></td><td><p>Low tier ~$0.01/image; Medium ~$0.04/image; High ~$0.17/image. </p></td><td><p>Token-based pricing – more complex prompts or higher resolution raise cost.</p></td></tr><tr><td><p>Google – Gemini 2.5 Flash Image (Nano Banana)</p></td><td><p>~$0.039 per image for 1024×1024 resolution (1290 tokens) in output. </p></td><td><p>Lower cost “flash” model for high-volume, lower latency use.</p></td></tr><tr><td><p>Other / Smaller APIs (e.g., via third-party credit systems)</p></td><td><p>Examples: $0.02–$0.03 per image in some cases for lower resolution or simpler models. </p></td><td><p>Often used for less demanding production use cases or draft content.</p></td></tr></tbody></table><p>The Google Gemini 3 Pro Image <!-- -->/ Nano Banana Pro<!-- --> pricing sits at the upper end: ~$0.134 for 1K/2K, ~$0.24 for 4K,  significantly higher than the ~$0.04 per image baseline for many OpenAI/DALL-E 3 standard images. </p><p>But the higher cost might be justifiable if: you require 4K resolution; you need enterprise-grade governance (e.g., Google emphasizes that paid-tier images are <i>not</i> used to train their systems); you need a token-based pricing system aligned with other LLM usage; and you already operate within Google’s cloud/AI stack (e.g., using Vertex AI).</p><p>On the other hand, if you’re generating large volumes of images (thousands to tens of thousands) and can accept lower resolution (1K/2K) or slightly less premium quality, the lower-cost alternatives (OpenAI, smaller models) offer meaningful savings — for instance, generating 10,000 images at ~$0.04 each costs ~$400, whereas at ~$0.134 each it’s ~$1,340. Over time, that delta adds up.</p><h2><b>SynthID and the Growing Need for Enterprise Provenance</b></h2><p>Every image generated by Gemini 3 Pro Image includes SynthID, Google’s imperceptible digital watermarking system. While many platforms are just beginning to explore AI provenance, Google is positioning SynthID as a core part of its enterprise compliance stack.</p><p>In the updated Gemini app, users can now upload an image and ask whether it was AI-generated by Google—a feature designed to support growing regulatory and internal governance demands.</p><p>A Google blog post emphasizes that provenance is no longer a “feature” but an operational requirement, particularly in high-stakes domains like healthcare, education, and media. SynthID also allows teams building on Google Cloud to differentiate between AI-generated content and third-party media across assets, use logs, and audit trails.</p><h2><b>Early Developer Reactions Range from Awe to Edge-Case Testing</b></h2><p>Despite the enterprise framing, early developer reactions have turned social media into a real-time proving ground.</p><p>Designer <a href=\"https://x.com/MrDavids1/status/1991514343666753870\">Travis Davids</a> called out a one-shot restaurant menu with flawless layout and typography: “Long generated text is officially solved.” </p><p>Immunologist <a href=\"https://x.com/DeryaTR_/status/1991541200147607939\">Dr. Derya Unutmaz</a> posted his CAR-T diagram with the caption: “What have you done, Google?!” while<a href=\"https://x.com/nikunj/status/1991550373249876286\"> Nikunj Kothari </a>converted a full essay into a stylized blackboard lecture in one shot, calling the results “simply speechless.”</p><div></div><p>Engineer <a href=\"https://x.com/deedydas/status/1991525559332139041?s=20\">Deedy Das</a> praised its performance across editing and brand restoration tasks: “Photoshop-like editing… It nails everything...By far the best image model I&#x27;ve ever seen.” </p><div></div><p>Developer<a href=\"https://x.com/ParkerOrtolani/status/1991321405728911660\"> Parker Ortolani </a>summarized it more simply: “Nano Banana remains absolutely bonkers.”</p><p>Even meme creators got involved. <a href=\"https://x.com/cto_junior/status/1991564259516702997\">@cto_junior </a>generated a fully styled “LLM discourse desk” meme—with logos, charts, monitors, and all—in one prompt, dubbing Gemini 3 Pro Image “your new meme engine.”</p><p>But scrutiny followed, too. AI researcher <a href=\"https://x.com/scaling01/status/1991553936202063937\">Lisan al Gaib</a> tested the model on a logic-heavy Sudoku problem, showing it hallucinated both an invalid puzzle and a nonsensical solution, noting that the model “is sadly not AGI.” </p><p>The post served as a reminder that visual reasoning has limits, particularly in rule-constrained systems where hallucinated logic remains a persistent failure mode.</p><h2><b>A New Platform Primitive, Not Just a Model</b></h2><p>Gemini 3 Pro Image now lives across Google’s entire enterprise and developer stack: Google Ads, Workspace (Slides, Vids), Vertex AI, Gemini API, and Google AI Studio. It’s also deployed in internal tools like Antigravity, where design agents render layout drafts before interface elements are coded.</p><p>This makes it a first-class multimodal primitive inside Google’s AI ecosystem, much like text completion or speech recognition. </p><p>In enterprise applications, visuals are not decorations—they’re data, documentation, design, and communication. Whether generating onboarding explainers, prototype visuals, or localized collateral, models like Gemini 3 Pro Image allow systems to create assets programmatically, with control, scale, and consistency.</p><p>At a time when the race between OpenAI, Google, and xAI is moving beyond benchmarks and into platforms, Nano Banana Pro is Google’s quiet declaration: the future of generative AI won’t just be spoken or written—it will be seen.</p>",
      "source": "AI | VentureBeat",
      "source_url": "https://venturebeat.com/category/ai/feed/",
      "author": "carl.franzen@venturebeat.com (Carl Franzen)",
      "tags": [
        "AI"
      ],
      "relevance_score": 75,
      "category": "Production Use Cases"
    },
    {
      "title": "ScaleOps' new AI Infra Product slashes GPU costs for self-hosted enterprise LLMs by 50% for early adopters",
      "url": "https://venturebeat.com/ai/scaleops-new-ai-infra-product-slashes-gpu-costs-for-self-hosted-enterprise",
      "published": "2025-11-20T22:35:00+00:00",
      "date_verified": true,
      "description": "<p><a href=\"https://scaleops.com/\">ScaleOps</a> has expanded its cloud resource management platform with a new product aimed at enterprises operating self-hosted large language models (LLMs) and GPU-based AI applications. </p><p>The <a href=\"https://www.prnewswire.com/il/news-releases/scaleops-launches-ai-infrastructure-resource-management-product-to-power-self-hosted-ai-at-scale-302621807.html\">AI Infra Product announced today</a>, extends the company’s existing automation capabilities to address a growing need for efficient GPU utilization, predictable performance, and reduced operational burden in large-scale AI deployments. </p><p>The company said the system is already running in enterprise production environments and delivering major efficiency gains for early adopters, reducing GPU costs by between 50% and 70%, according to the company. The company does not publicly list enterprise pricing for this solution and instead invites interested customers to receive a custom quote based on their operation size and needs <a href=\"https://scaleops.com/pricing/\">here</a>.</p><p>In explaining how the system behaves under heavy load, Yodar Shafrir, CEO and Co-Founder of ScaleOps, said in an email to VentureBeat that the platform uses “proactive and reactive mechanisms to handle sudden spikes without performance impact,” noting that its workload rightsizing policies “automatically manage capacity to keep resources available.” </p><p>He added that minimizing GPU cold-start delays was a priority, emphasizing that the system “ensures instant response when traffic surges,” particularly for AI workloads where model load times are substantial.</p><h2><b>Expanding Resource Automation to AI Infrastructure</b></h2><p>Enterprises deploying self-hosted AI models face performance variability, long load times, and persistent underutilization of GPU resources. ScaleOps positioned the new AI Infra Product as a direct response to these issues. </p><p>The platform allocates and scales GPU resources in real time and adapts to changes in traffic demand without requiring alterations to existing model deployment pipelines or application code.</p><p>According to ScaleOps, the system manages production environments for organizations including Wiz, DocuSign, Rubrik, Coupa, Alkami, Vantor, Grubhub, Island, Chewy, and several Fortune 500 companies. </p><p>The AI Infra Product introduces workload-aware scaling policies that proactively and reactively adjust capacity to maintain performance during demand spikes. The company stated that these policies reduce the cold-start delays associated with loading large AI models, which improves responsiveness when traffic increases.</p><h2><b>Technical Integration and Platform Compatibility</b></h2><p>The product is designed for compatibility with common enterprise infrastructure patterns. It works across all Kubernetes distributions, major cloud platforms, on-premises data centers, and air-gapped environments. ScaleOps emphasized that deployment does not require code changes, infrastructure rewrites, or modifications to existing manifests. </p><p>Shafrir said the platform “integrates seamlessly into existing model deployment pipelines without requiring any code or infrastructure changes,” and he added that teams can begin optimizing immediately with their existing GitOps, CI/CD, monitoring, and deployment tooling.</p><p>Shafrir also addressed how the automation interacts with existing systems. He said the platform operates without disrupting workflows or creating conflicts with custom scheduling or scaling logic, explaining that the system “doesn’t change manifests or deployment logic” and instead enhances schedulers, autoscalers, and custom policies by incorporating real-time operational context while respecting existing configuration boundaries.</p><h2><b>Performance, Visibility, and User Control</b></h2><p>The platform provides full visibility into GPU utilization, model behavior, performance metrics, and scaling decisions at multiple levels, including pods, workloads, nodes, and clusters. While the system applies default workload scaling policies, ScaleOps noted that engineering teams retain the ability to tune these policies as needed.</p><p>In practice, the company aims to reduce or eliminate the manual tuning that DevOps and AIOps teams typically perform to manage AI workloads. Installation is intended to require minimal effort, described by ScaleOps as a two-minute process using a single helm flag, after which optimization can be enabled through a single action.</p><h2><b>Cost Savings and Enterprise Case Studies</b></h2><p>ScaleOps reported that early deployments of the AI Infra Product have achieved GPU cost reductions of 50–70% in customer environments. The company cited two examples:</p><ul><li><p>A major creative software company operating thousands of GPUs averaged 20% utilization before adopting ScaleOps. The product increased utilization, consolidated underused capacity, and enabled GPU nodes to scale down. These changes reduced overall GPU spending by more than half. The company also reported a 35% reduction in latency for key workloads.</p></li><li><p>A global gaming company used the platform to optimize a dynamic LLM workload running on hundreds of GPUs. According to ScaleOps, the product increased utilization by a factor of seven while maintaining service-level performance. The customer projected $1.4 million in annual savings from this workload alone.</p></li></ul><p>ScaleOps stated that the expected GPU savings typically outweigh the cost of adopting and operating the platform, and that customers with limited infrastructure budgets have reported fast returns on investment.</p><h2><b>Industry Context and Company Perspective</b></h2><p>The rapid adoption of self-hosted AI models has created new operational challenges for enterprises, particularly around GPU efficiency and the complexity of managing large-scale workloads. Shafrir described the broader landscape as one in which “cloud-native AI infrastructure is reaching a breaking point.”</p><p>“Cloud-native architectures unlocked great flexibility and control, but they also introduced a new level of complexity,” he said in the announcement. “Managing GPU resources at scale has become chaotic—waste, performance issues, and skyrocketing costs are now the norm. The ScaleOps platform was built to fix this. It delivers the complete solution for managing and optimizing GPU resources in cloud-native environments, enabling enterprises to run LLMs and AI applications efficiently, cost-effectively, and while improving performance.”</p><p>Shafrir added that the product brings together the full set of cloud resource management functions needed to manage diverse workloads at scale. The company positioned the platform as a holistic system for continuous, automated optimization.</p><h2><b>A Unified Approach for the Future</b></h2><p>With the addition of the AI Infra Product, ScaleOps aims to establish a unified approach to GPU and AI workload management that integrates with existing enterprise infrastructure. </p><p>The platform’s early performance metrics and reported cost savings suggest a focus on measurable efficiency improvements within the expanding ecosystem of self-hosted AI deployments.</p>",
      "source": "AI | VentureBeat",
      "source_url": "https://venturebeat.com/category/ai/feed/",
      "author": "carl.franzen@venturebeat.com (Carl Franzen)",
      "tags": [
        "AI",
        "Data Infrastructure"
      ],
      "relevance_score": 75,
      "category": "Production Use Cases"
    },
    {
      "title": "Nano Banana Pro aka gemini-3-pro-image-preview is the best available image generation model",
      "url": "https://simonwillison.net/2025/Nov/20/nano-banana-pro/#atom-everything",
      "published": "2025-11-20T21:32:25+00:00",
      "date_verified": true,
      "description": "<p>Hot on the heels of Tuesday's <a href=\"https://simonwillison.net/2025/Nov/18/gemini-3/\">Gemini 3 Pro</a> release, today it's <a href=\"https://blog.google/technology/ai/nano-banana-pro/\">Nano Banana Pro</a>, also known as <a href=\"https://deepmind.google/models/gemini-image/pro/\">Gemini 3 Pro Image</a>. I've had a few days of preview access and this is an <em>astonishingly</em> capable image generation model.</p>\n<p>As is often the case, the most useful low-level details can be found in <a href=\"https://ai.google.dev/gemini-api/docs/image-generation#gemini-3-capabilities\">the API documentation</a>:</p>\n<blockquote>\n<p>Designed to tackle the most challenging workflows through advanced reasoning, it excels at complex, multi-turn creation and modification tasks.</p>\n<ul>\n<li>\n<strong>High-resolution output</strong>: Built-in generation capabilities for 1K, 2K, and 4K visuals.</li>\n<li>\n<strong>Advanced text rendering</strong>: Capable of generating legible, stylized text for infographics, menus, diagrams, and marketing assets.</li>\n<li>\n<strong>Grounding with Google Search</strong>: The model can use Google Search as a tool to verify facts and generate imagery based on real-time data (e.g., current weather maps, stock charts, recent events).</li>\n<li>\n<strong>Thinking mode</strong>: The model utilizes a \"thinking\" process to reason through complex prompts. It generates interim \"thought images\" (visible in the backend but not charged) to refine the composition before producing the final high-quality output.</li>\n<li>\n<strong>Up to 14 reference images</strong>: You can now mix up to 14 reference images to produce the final image.</li>\n</ul>\n<p>[...] These 14 images can include the following:</p>\n<ul>\n<li>Up to 6 images of objects with high-fidelity to include in the final image</li>\n<li>Up to 5 images of humans to maintain character consistency</li>\n</ul>\n</blockquote>\n<p>There is also a short (6 page) <a href=\"https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Image-Model-Card.pdf\">model card PDF</a> which lists the following as \"new capabilities\" compared to the previous Nano Banana: Multi character editing, Chart editing, Text editing, Factuality - Edu, Multi-input 1-3, Infographics, Doodle editing, Visual design.</p>\n<h4 id=\"trying-out-some-detailed-instruction-image-prompts\">Trying out some detailed instruction image prompts</h4>\n<p>Max Woolf published <a href=\"https://minimaxir.com/2025/11/nano-banana-prompts/#hello-nano-banana\">the definitive guide to prompting Nano Banana</a> just a few days ago. I decided to try his example prompts against the new model, requesting results in 4K.</p>\n<p>Here's what I got for his first test prompt, using Google's <a href=\"https://aistudio.google.com/\">AI Studio</a>:</p>\n<blockquote>\n<p><code>Create an image of a three-dimensional pancake in the shape of a skull, garnished on top with blueberries and maple syrup.</code></p>\n</blockquote>\n<p><img alt=\"A very detailed quality photo of a skull made of pancake batter, blueberries on top, maple syrup dripping down, maple syrup bottle in the background.\" src=\"https://static.simonwillison.net/static/2025/pancake-skull-1.jpg\" /></p>\n<p>The result came out as a 24.1MB, 5632 × 3072 pixel PNG file. I don't want to serve that on my own blog so here's <a href=\"https://drive.google.com/file/d/1QV3pcW1KfbTRQscavNh6ld9PyqG4BRes/view?usp=drive_link\">a Google Drive link for the original</a>.</p>\n<p>Then I ran his follow-up prompt:</p>\n<pre><code>Make ALL of the following edits to the image:\n- Put a strawberry in the left eye socket.\n- Put a blackberry in the right eye socket.\n- Put a mint garnish on top of the pancake.\n- Change the plate to a plate-shaped chocolate-chip cookie.\n- Add happy people to the background.\n</code></pre>\n<p><img alt=\"It's the exact same skull with the requested edits made - mint garnish on the blueberries, a strawberry in the left hand eye socket (from our perspective, technically the skull's right hand socket), a blackberry in the other, the plate is now a plate-sized chocolate chip cookie (admittedly on a regular plate) and there are four happy peo ple in the background.\" src=\"https://static.simonwillison.net/static/2025/pancake-skull-2.jpg\" /></p>\n<p>I'll note that it did put the plate-sized cookie on a regular plate. Here's <a href=\"https://drive.google.com/file/d/18AzhM-BUZAfLGoHWl6MQW_UW9ju4km-i/view?usp=drive_link\">the 24.9MB PNG</a>.</p>\n<p>The new model isn't cheap. Here's <a href=\"https://ai.google.dev/gemini-api/docs/pricing#gemini-3-pro-image-preview\">the API pricing</a>: it's 24 cents for a 4K image and 13.4 cents for a 1K or 2K image. Image inputs are 0.11 cents (just over 1/10th of a cent) each - an earlier version of their pricing page incorrectly said 6.7 cents each but that's now been fixed.</p>\n<p>Unlike most of Google's other models it also isn't available for free via AI Studio: you have to configure an API key with billing in order to use the model there.</p>\n<h4 id=\"creating-an-infographic\">Creating an infographic</h4>\n<p>So this thing is great at following instructions. How about rendering text?</p>\n<p>I tried this prompt, this time using the Gemini consumer app in \"thinking\" mode (which now uses Nano Banana Pro for image generation). <a href=\"https://gemini.google.com/share/d40fe391f309\">Here's a share link</a> - my prompt was:</p>\n<blockquote>\n<p>Infographic explaining how the Datasette open source project works</p>\n</blockquote>\n<p>This is a great opportunity to test its ability to run searches (aka \"Grounding with Google Search\"). Here's what it created based on that 9 word prompt:</p>\n<p><img alt=\"Described by Gemini 3 Pro: A technical architecture diagram titled &quot;How Datasette Works: From Raw Data to Explorable API,&quot; illustrating a workflow from left to right. 1. &quot;RAW DATA SOURCES&quot; include &quot;CSV&quot;, &quot;JSON&quot;, &quot;Excel (XLSX)&quot;, and &quot;Log Files&quot;. 2. These flow into &quot;DATA PREPARATION &amp; CONVERSION&quot; using tools &quot;csvs-to-sqlite&quot; and &quot;sqlite-utils&quot; to create a &quot;SQLite DATABASE&quot;. 3. This feeds into the central &quot;DATASETTE APPLICATION CORE,&quot; a stack comprising &quot;Data Ingestion (Read-Only)&quot;, &quot;Query Engine (SQL)&quot;, &quot;API Layer (JSON)&quot;, and &quot;Web UI Rendering&quot;. 4. A &quot;PLUGIN ECOSYSTEM&quot; connects below the core, listing &quot;Vega Charts&quot;, &quot;Cluster Maps&quot;, &quot;Full-Text Search&quot;, and &quot;Custom Renderers&quot;. 5. To the right, the core branches into &quot;OUTPUT &amp; INTERFACES&quot;: an &quot;Interactive Web Interface (Explore, Filter, Facet)&quot;, a &quot;JSON API (Programmatic Access)&quot; showing a snippet {&quot;data&quot;: [...]}, and a &quot;SQL EDITOR Custom SQL Queries&quot; showing SELECT * FROM.... 6. The API output connects to &quot;PUBLISHING &amp; DEPLOYMENT&quot; via a terminal command datasette publish cloudrun my.db leading to deployment targets &quot;Heroku&quot;, &quot;Google Cloud Run&quot;, &quot;Fly.io&quot;, and &quot;Vercel&quot;.\" src=\"https://static.simonwillison.net/static/2025/nano-banana-datasette.jpg\" /></p>\n<p>That's a pretty extraordinary result for such a simple prompt. The text is all spelled correctly and rendered without glitches. The content is solid too - it even included logos for the most popular publish platforms, and a tiny thumbnail of the Datasette UI which is close-enough for an infographic.</p>\n<p>\"Data Ingestion (Read-Only)\" is a bit off. Everything else looks great to me.</p>\n<p>Here's the thinking trace for that image generation process:</p>\n<blockquote>\n<p><strong>Initiating Datasette Infographic</strong></p>\n<p>I'm starting a detailed infographic about the Datasette project. I plan to use Google Search to gather specific information about its core processes, which I will then visualize in a multi-panel diagram.</p>\n<p><strong>Developing the Datasette workflow</strong></p>\n<p>I am visualizing the Datasette workflow as described in the search results. I'm focusing on the conversion of data from different formats into a SQLite database, which is then presented as a web application complete with a user interface and JSON output.</p>\n<p><strong>Visualizing Datasette Workflow</strong></p>\n<p>I've sketched out the initial structure for the infographic, focusing on a flowchart style. The key components include data input from diverse sources, conversion to SQLite, the Datasette core application, its main features (web UI, JSON API, SQL querying), ecosystem plugins, and finally, deployment options. I aim for clear icons and labels to represent each step. Now I will focus on refining the flow and clarifying the relationships between the parts.</p>\n<p><strong>Defining Core Datasette Stages</strong></p>\n<p>I've sketched out the initial infographic structure. I have identified the key components, focusing on visual representation of the data flow and the key benefits and features of the system. I have made updates to the flowchart with clear icons and labels. Now I'm working to expand the data input section and refine the visual representation of each step.</p>\n<p><strong>Constructing Datasette Infographic</strong></p>\n<p>I am translating the current Datasette workflow outline into visual elements. The input section will show icons for different data formats. I've sketched the data conversion stage with clear illustrations of the necessary tools. I will add a depiction of the Datasette core application, showcasing its interfaces and functions. The plugin ecosystem and deployment options will follow. Now, I am refining the visual representation.</p>\n<p><strong>Analyzing Datasette's Architecture</strong></p>\n<p>I'm currently working on representing the Datasette open-source project's architecture visually. I've broken it down into key components: raw data processing, SQLite conversion, core application functionalities, plugin integration, and deployment methods. I'm focusing on creating a clear, linear representation to show the workflow.</p>\n<p><strong>Mapping the Datasette Flow</strong></p>\n<p>I've crafted a comprehensive graphic to illustrate Datasette's operation. It spans the data ingestion, SQLite conversion, main application features, plugin interactions, and deployment stages, aiming for clarity and a logical flow.</p>\n</blockquote>\n<h4 id=\"detecting-generated-images-with-synthid\">Detecting generated images with SynthID</h4>\n<p>This note from the <a href=\"https://blog.google/technology/ai/nano-banana-pro/\">announcement post</a> caught my eye:</p>\n<blockquote>\n<p>We believe it's critical to know when an image is AI-generated. This is why all media generated by Google's tools are embedded with our imperceptible <a href=\"https://deepmind.google/models/synthid/\">SynthID</a> digital watermark.</p>\n<p>Today, we are putting a powerful verification tool directly in consumers' hands: you can now upload an image into the Gemini app and simply ask if it was generated by Google AI, thanks to SynthID technology. We are starting with images, but will expand to audio and video soon.</p>\n</blockquote>\n<p>Last night I used Nano Banana Pro to generate a fake photograph of raccoons stealing our food delivery, then scrubbed out the little diamond icon using the Apple Photos \"cleanup\" tool. I uploaded that Gemini app and asked \"Was this image created with AI?\":</p>\n<p><img alt=\"Screenshot of a mobile chat interface displaying a conversation about AI image detection. The user has uploaded a photo showing two raccoons on a porch; one raccoon reaches inside a paper bag a bench while the other stands on the ground looking up at it. The conversation title reads &quot;AI Image Creation Confirmed&quot;. The user asks, &quot;Was this image created with ai?&quot; The AI response, labeled &quot;Analysis &amp; 1 more&quot;, states: &quot;Yes, it appears that all or part of this image was created with Google AI. SynthID detected a watermark in 25-50% of the image.&quot;\" src=\"https://static.simonwillison.net/static/2025/nano-banana-detected.jpg\" /></p>\n<p>It replied:</p>\n<blockquote>\n<p>Yes, it appears that all or part of this image was created with Google Al. SynthID detected a watermark in 25-50% of the image.</p>\n</blockquote>\n<p>Presumably that 25-50% figure is because the rest of the photo was taken by me - it was just the raccoons that were added by Nano Banana Pro.</p>\n    \n        <p>Tags: <a href=\"https://simonwillison.net/tags/google\">google</a>, <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/datasette\">datasette</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/gemini\">gemini</a>, <a href=\"https://simonwillison.net/tags/text-to-image\">text-to-image</a>, <a href=\"https://simonwillison.net/tags/llm-release\">llm-release</a>, <a href=\"https://simonwillison.net/tags/nano-banana\">nano-banana</a></p>",
      "source": "Simon Willison's Weblog",
      "source_url": "https://simonwillison.net/atom/everything/",
      "author": "",
      "tags": [
        "google",
        "ai",
        "datasette",
        "generative-ai",
        "llms",
        "gemini",
        "text-to-image",
        "llm-release",
        "nano-banana"
      ],
      "relevance_score": 75,
      "category": "Production Use Cases"
    },
    {
      "title": "Tome's founders ditch viral presentation app with 20M users to build AI-native CRM Lightfield",
      "url": "https://venturebeat.com/ai/tomes-founders-ditch-viral-presentation-app-with-20m-users-to-build-ai",
      "published": "2025-11-20T19:00:00+00:00",
      "date_verified": true,
      "description": "<p><a href=\"https://lightfield.app/\"><u>Lightfield</u></a>, a customer relationship management platform built entirely around artificial intelligence, officially launched to the public this week after a year of quiet development — a bold pivot by a startup that once had <a href=\"https://www.forbes.com/sites/rashishrivastava/2024/04/23/the-prompt-the-latest-ai-startup-to-face-reality/\"><u>20 million users</u></a> and <a href=\"https://www.forbes.com/sites/alexkonrad/2023/02/22/storytelling-ai-startup-tome-raises-43-million/\"><u>$43 million in the bank</u></a> building something completely different.</p><p>The San Francisco-based company is positioning itself as a fundamental reimagining of how businesses track and manage customer relationships, abandoning the manual data entry that has defined CRMs for decades in favor of a system that automatically captures, organizes, and acts on customer interactions. With more than 100 early customers already using the platform daily — over half spending more than an hour per day in the system — <a href=\"https://lightfield.app/\"><u>Lightfield</u></a> is a direct challenge to the legacy business models of <a href=\"https://www.salesforce.com/\"><u>Salesforce</u></a> and <a href=\"https://www.hubspot.com/\"><u>HubSpot</u></a>, both of which generate billions in annual revenue.</p><p>&quot;The CRM, categorically, is perhaps the most complex and lowest satisfaction piece of software on Earth,&quot; said Keith Peiris, Lightfield&#x27;s co-founder and CEO, in an exclusive interview with VentureBeat. &quot;CRM companies have tens of millions of users, and you&#x27;d be hard-pressed to find a single one who actually loves the product. That problem is our opportunity.&quot;</p><p>The <a href=\"https://lightfield.app/\"><u>general availability</u></a> announcement marks an unusual inflection point in enterprise software: a company betting that large language models have advanced enough to replace structured databases as the foundation of business-critical systems. It&#x27;s a wager that has attracted backing from <a href=\"https://www.coatue.com/\"><u>Coatue Management</u></a>, which led the company&#x27;s Series A when it was still building presentation software under the name Tome.</p><h2><b>How Tome&#x27;s founders abandoned 20 million users to build a CRM from scratch</b></h2><p>The story behind Lightfield&#x27;s creation reflects both conviction and pragmatism. <a href=\"https://www.fastcompany.com/90827339/presentation-tool-tome-launches-ai-to-help-make-storytelling-simpler\"><u>Tome</u></a> had achieved significant viral success as an AI-powered presentation platform, gaining millions of users who appreciated its visual design and ease of use. But Peiris said the team concluded that building lasting differentiation in the general-purpose presentation market would prove difficult, even with a working product and real user traction.</p><p>&quot;Tome went viral as an AI slides product, and it was visually delightful and easy to use—the first real generative AI-based presentation platform,&quot; Peiris explained. &quot;But, the more people used it, the more I realized that to really help people communicate something—anything—we needed more context.&quot;</p><p>That realization led to a fundamental rethinking. The team observed that the most effective communication requires deep understanding of relationships, company dynamics, and ongoing conversations — context that exists most richly in sales and customer-facing roles. Rather than building a horizontal tool for everyone, they decided to build vertically for go-to-market teams.</p><p>&quot;We chose this lane, &#x27;sales,&#x27; because so many people in these roles used Tome, and it seemed like the most logical place to go vertical,&quot; Peiris said. The team reduced headcount to a core group of engineers and spent a year building in stealth.</p><p><a href=\"https://signal.nfx.com/investors/dan-rose\"><u>Dan Rose</u></a>, a senior advisor at <a href=\"https://www.coatue.com/\"><u>Coatue</u></a> who led the original investment in Tome, said the pivot validated his conviction in the founding team. &quot;It takes real guts to pivot, and even more so when the original product is working,&quot; Rose said. &quot;They shrunk the team down to a core group of engineers and got to work building Lightfield. This was not an easy product to build, it is extremely complex under the hood.&quot;</p><h2><b>Why Lightfield stores complete conversations instead of forcing data into fields</b></h2><p>What distinguishes <a href=\"https://lightfield.app/\"><u>Lightfield</u></a> from traditional CRMs is architectural, not cosmetic. While <a href=\"https://www.salesforce.com/\"><u>Salesforce</u></a>, <a href=\"https://www.hubspot.com/\"><u>HubSpot</u></a>, and their competitors require users to define rigid data schemas upfront — dropdown menus, custom fields, checkbox categories — and then manually populate those fields after every interaction, Lightfield stores the complete, unstructured record of what customers actually say and do.</p><p>&quot;Traditional CRMs force every interaction through predefined fields — they&#x27;re compressing rich, nuanced customer conversations into structured database entries,&quot; Peiris said. &quot;We store customer data in its raw, lossless form. That means we&#x27;re capturing significantly more detail and context than a traditional CRM ever could.&quot;</p><p>In practice, this means the system automatically records and transcribes sales calls, ingests emails, monitors product usage, and maintains what the company calls a &quot;relationship timeline&quot; — a complete chronological record of every touchpoint between a company and its customers. AI models then extract structured information from this raw data on demand, allowing companies to reorganize their data model without manual rework.</p><p>&quot;If you realize you need different fields or want to reorganize your schema entirely, the system can remap and refill itself automatically,&quot; Peiris explained. &quot;You&#x27;re not locked into decisions you made on day one when you barely understood your sales process.&quot;</p><p>The system also generates meeting preparation briefs, drafts follow-up emails based on conversation context, and can be queried in natural language — capabilities that represent a departure from the passive database model that has defined CRMs since the category&#x27;s inception in the 1980s.</p><h2><b>Sales teams report reviving dead deals and cutting response times from months to days</b></h2><p>Customer testimonials suggest the automation delivers measurable impact, particularly for small teams without dedicated sales operations staff. Tyler Postle, co-founder of <a href=\"http://voker.ai\"><u>Voker.ai</u></a>, said Lightfield&#x27;s AI agent helped him revive more than 40 stalled opportunities in a single two-hour session — leads he had neglected for six months while using HubSpot.</p><p>&quot;Within 2 days, 10 of those were revived and became active opps that moved to poc,&quot; Postle said. &quot;The problem was, instead of being a tool of action and autotracking—HubSpot was a tool where I had to do the work to record customer convos. Using HubSpot I was a data hygienist. Using Lighfield, I’m a closer.&quot;</p><p>Postle reported that his response times to prospects improved from weeks or months to one or two days, a change noticeable enough that customers commented on it. &quot;Our prospects and customers have even noticed it,&quot; he said.</p><p>Radu Spineanu, co-founder of <a href=\"https://humbleops.ai/\"><u>Humble Ops</u></a>, highlighted a specific feature that addresses what he views as the primary cause of lost deals: simple neglect. &quot;The killer feature is asking &#x27;who haven&#x27;t I followed up with?&#x27;&quot; Spineanu said. &quot;Most deals die from neglect, not rejection. Lightfield catches these dropped threads and can draft and send the follow-up immediately. That&#x27;s prevented at least three deals from going cold this quarter.&quot;</p><p>Spineanu had evaluated competing modern CRMs including <a href=\"https://attio.com/\"><u>Attio</u></a> and <a href=\"https://www.clay.com/\"><u>Clay</u></a> before selecting <a href=\"https://lightfield.app/\"><u>Lightfield</u></a>, dismissing <a href=\"https://www.salesforce.com/\"><u>Salesforce</u></a> and <a href=\"https://www.hubspot.com/\"><u>HubSpot</u></a> as &quot;built for a different era.&quot; He said those platforms assume companies have dedicated operations teams to configure workflows and maintain data quality — resources most early-stage companies lack.</p><h2><b>Why Y Combinator startups are rejecting Salesforce and starting with AI-native tools</b></h2><p>Peiris claims that the current batch of <a href=\"https://www.ycombinator.com/\"><u>Y Combinator</u></a> startups — widely viewed as a bellwether for early-stage company behavior — have largely rejected both <a href=\"https://www.salesforce.com/\"><u>Salesforce</u></a> and <a href=\"https://www.hubspot.com/\"><u>HubSpot</u></a>. &quot;If you were to poll a random sampling of current YC startups and ask whether they&#x27;re using Salesforce or HubSpot, the overwhelming answer would be &#x27;no,&#x27;&quot; he said. &quot;Salesforce is too expensive, too complex to set up, and frankly doesn&#x27;t do enough to justify the investment for an early-stage company.&quot;</p><p>According to Peiris, most startups begin with spreadsheets and eventually graduate to a first CRM — a transition point where Lightfield aims to intercede. &quot;Increasingly, they&#x27;re choosing Lightfield instead and skipping that intermediate step entirely,&quot; he said.</p><p>This represents a familiar pattern in enterprise software disruption: a new generation of companies forming habits around different tools, creating an opening for challengers to establish themselves before businesses grow large enough to face pressure toward industry-standard platforms. The company&#x27;s strategy appears to deliberately target this window, aiming to grow alongside early customers and become embedded in their processes as they scale.</p><h2><b>Can Salesforce and HubSpot retrofit their legacy systems for AI, or is the architecture too old?</b></h2><p>Both <a href=\"https://www.salesforce.com/\"><u>Salesforce</u></a> and <a href=\"https://www.hubspot.com/\"><u>HubSpot</u></a> have announced AI features in recent quarters, adding capabilities like conversation intelligence and automated data entry to their existing platforms. The question facing Lightfield is whether established vendors can incorporate similar capabilities—leveraging their existing customer bases and integrations — or whether fundamental architectural differences create a genuine moat.</p><p>Peiris argues the latter. &quot;The fundamental difference is in how we store data,&quot; he said. &quot;Because we have access to that complete context, the analysis we provide and the work we generate tends to be substantially higher quality than tools built on top of traditional database structures.&quot;</p><p>Existing conversation intelligence tools like <a href=\"https://www.gong.io/\"><u>Gong</u></a> and <a href=\"http://revenue.io\"><u>Revenue.io</u></a>, which analyze sales calls and provide coaching insights, already serve similar functions but require Salesforce instances to operate. Peiris said Lightfield&#x27;s advantage comes from unifying the entire data model rather than layering analysis on top of fragmented systems.</p><p>&quot;We have a more complete picture of each customer because we integrate company knowledge, communication sync, product analytics, and full CRM detail all in one place,&quot; he said. &quot;That unified context means the work being generated in Lightfield—whether it&#x27;s analysis, follow-ups, or insights—tends to be significantly higher quality.&quot;</p><h2><b>The privacy and accuracy concerns that come with AI-automated customer interactions</b></h2><p>The architecture creates obvious risks. Storing complete conversation histories raises privacy concerns, and relying on large language models to extract and interpret information introduces the possibility of errors—what AI researchers call hallucinations.</p><p>Peiris acknowledged both issues directly. On privacy, the company maintains that call recording follows standard practices, with visible notifications that recording is in progress, and that storing sales correspondence mirrors what CRM vendors have done for decades. The company has achieved SOC 2 Type I certification and is pursuing both SOC 2 Type II and HIPAA compliance. &quot;We don&#x27;t train models on customer data, period,&quot; Peiris said.</p><p>On accuracy, he was similarly forthright. &quot;Of course it happens,&quot; Peiris said when asked about misinterpretations. &quot;It&#x27;s impossible to completely eliminate hallucinations when working with large language models.&quot;</p><p>The company&#x27;s approach is to require human approval before sending customer communications or updating critical fields — positioning the system as augmentation rather than full automation. &quot;We&#x27;re building a tool that amplifies human judgment, not one that pretends to replace it entirely,&quot; Peiris said.</p><p>This is a more cautious stance than some AI-native software companies have taken, reflecting both technical realism about current model capabilities and potential liability concerns around customer-facing mistakes.</p><h2><b>How Lightfield plans to consolidate ten different sales tools into one platform</b></h2><p>Lightfield&#x27;s pricing strategy reflects a broader thesis about enterprise software economics. Rather than charging per-seat fees for a point solution, the company is positioning itself as a consolidated platform that can replace multiple specialized tools — sales engagement platforms, conversation intelligence systems, meeting assistants, and the CRM itself.</p><p>&quot;The real problem is that running a modern go-to-market function requires cobbling together 10 different independent point solutions,&quot; Peiris said. &quot;When you pay for 10 separate seat licenses, you&#x27;re essentially paying 10 different companies to solve the same foundational problems over and over again.&quot;</p><p>The company operates primarily through self-service signup rather than enterprise sales teams, which Peiris argues allows for lower pricing while maintaining margins. This is a common playbook among modern SaaS companies but represents a fundamental difference from Salesforce&#x27;s model, which relies heavily on direct sales and customer success teams.</p><p>Whether this approach can support a sustainable business at scale remains unproven. The company&#x27;s current customer base skews heavily toward early-stage startups—more than 100 <a href=\"https://www.ycombinator.com/\"><u>Y Combinator</u></a> companies, according to the company — a segment with limited budgets and high failure rates.</p><p>But Lightfield is betting it can become the system of record for a cohort of fast-growing companies, eventually creating an installed base comparable to how Salesforce established itself decades ago. The company&#x27;s trajectory will likely depend on whether AI capabilities alone provide sufficient differentiation—or whether incumbents can adapt quickly enough to defend their positions.</p><h2><b>The real test: whether sales teams will trust AI enough to let it run their business</b></h2><p>The company has outlined several areas for expansion, including an open platform for workflows and webhooks that would allow third-party integrations. Early customers have specifically requested connections with tools like <a href=\"https://www.apollo.io/\"><u>Apollo</u></a> for prospecting and <a href=\"https://slack.com/\"><u>Slack</u></a> for team communication — gaps that Postle, the <a href=\"http://voker.ai\"><u>Voker.ai</u></a> founder, acknowledged but dismissed as temporary.</p><p>&quot;The fact that HS and Salesforce have these integrations already isn&#x27;t a moat,&quot; Postle said. &quot;HS and Salesforce are going to lose to lightfield because they aren&#x27;t AI native, no matter how much they try to pretend to be.&quot;</p><p>Rose highlighted an unusual use case that emerged during Lightfield&#x27;s own development: the company&#x27;s product team used the CRM itself to analyze customer conversations and identify feature requests. &quot;In this sense, Lightfield more than just a sales database, it&#x27;s a customer intelligence layer,&quot; Rose said.</p><p>This suggests potential applications beyond traditional sales workflows, positioning the system as infrastructure for any function that requires understanding customer needs—product development, customer success, even marketing strategy.</p><p>For now, the company is focused on proving the core value proposition with early-stage companies. But the broader question Lightfield raises extends beyond CRM software specifically: whether AI capabilities have advanced sufficiently to replace structured databases as the foundation of enterprise systems, or whether the current generation of large language models remains too unreliable for business-critical functions.</p><p>The answer will likely emerge not from technical benchmarks but from customer behavior—whether sales teams actually trust AI-generated insights enough to base decisions on them, and whether the efficiency gains justify the inherent unpredictability of working with systems that approximate rather than calculate.</p><p><a href=\"https://lightfield.app/\"><u>Lightfield</u></a> is betting that the trade-off has already shifted in favor of approximation, at least for the millions of salespeople who currently view their CRM as an obstacle rather than an asset. Whether that bet proves correct will help define the next generation of enterprise software.</p>",
      "source": "AI | VentureBeat",
      "source_url": "https://venturebeat.com/category/ai/feed/",
      "author": "michael.nunez@venturebeat.com (Michael Nuñez)",
      "tags": [
        "AI",
        "Software"
      ],
      "relevance_score": 75,
      "category": "Frameworks & Tools"
    },
    {
      "title": "Ai2’s Olmo 3 family challenges Qwen and Llama with efficient, open reasoning and customization",
      "url": "https://venturebeat.com/ai/ai2s-olmo-3-family-challenges-qwen-and-llama-with-efficient-open-reasoning",
      "published": "2025-11-20T10:00:00+00:00",
      "date_verified": true,
      "description": "<p>The <a href=\"https://allenai.org/\"><u>Allen Institute for AI (Ai2)</u></a> hopes to take advantage of an increased demand for customized models and enterprises seeking more transparency from AI models with its latest release.</p><p>Ai2 made the latest addition to its Olmo family of large language models available to organizations, continuing to focus on openness and customization. </p><p>Olmo 3 has a longer context window, more reasoning traces and is better at coding than its previous iteration. This latest version, like the other Olmo releases, is open-sourced under the Apache 2.0 license. Enterprises will have complete transparency into and control over the training data and checkpointing. </p><p>Ai2 will release three versions of Olmo 3:</p><ul><li><p>Olmo 3- Think in both 7B and 32B are considered the flagship reasoning models for advanced research</p></li><li><p>Olmo 3- Base also in both parameters, which is ideal for programming, comprehension, math and long-context reasoning. Ai2 said this version is “ideal for continued pre-training or fine-tuning</p></li><li><p>Olmo 3-Instruct in 7B that is optimized for instruction following, multi-turn dialogue and tool use</p></li></ul><p>The company said Olmo 3- Think is the “first-ever fully open 32B thinking model that generates explicit reasoning-chain-style content.” Olmo-3 Think also has a long context window of 65,000 tokens, perfect for longer-running agentic projects or reasoning over longer documents. </p><p>Noah Smith, Ai2’s senior director of NLP research, told VentureBeat in an interview that many of its customers, from regulated enterprises to research institutions, want to use models that give them assurance about what went into the training. </p><p>“The releases from our friends in the tech world are very cool and super exciting, but there are a lot of people for whom data privacy control over what goes into the model, how the models train and other constraints on how the model can be used as front of mind,” said Smith. </p><p>Developers can access the models on Hugging Face and the Ai2 Playground. </p><p></p><h2>Transparency and customization</h2><p>Smith said models like Olmo 3, which the company believes any organization using its models has to have control over and mold in the way that best works for them.</p><p>“We don&#x27;t believe in one-size-fits-all solutions,” Smith said. It&#x27;s a known thing in the world of machine learning that if you try and build a model that solves all the problems, it ends up not being really the best model for any one problem. There aren&#x27;t formal proofs of that, but it&#x27;s a thing that old timers like me have kind of observed.”</p><p>He added that models with the ability to specialize “are maybe not as flash as getting high scores on math exams” but offer more flexibility for enterprises. </p><p>Olmo 3 allows enterprises to essentially retrain the model by adding to the data mix it learns from. The idea is that businesses can bring in their proprietary sources to guide the model in answering specific company queries. To help enterprises during this process, Ai2 added checkpoints from every major training phase. </p><p>Demand for model customization has grown as enterprises that cannot build their own LLMs want to create company-specific or industry-focused models. Startups like <a href=\"https://tfwiki.net/wiki/Arcee_(G1)\"><u>Arcee</u></a> have <a href=\"https://venturebeat.com/ai/arcee-opens-up-new-enterprise-focused-customizable-ai-model-afm-4-5b-trained-on-clean-rigorously-filtered-data\"><u>begun offering</u></a> enterprise-focused, customizable small models. </p><p>Models like Olmo 3, Smith said, also give enterprises more confidence in the technology. Since Olmo 3 provides the training data, Smith said enterprises can trust that the model did not ingest anything it shouldn’t have.</p><p>Ai2 has always claimed to be committed to greater transparency, even launching a tool called <a href=\"https://venturebeat.com/ai/whats-inside-the-llm-ai2-olmotrace-will-trace-the-source\"><u>OlmoTrace in April</u></a> that can track a model’s output directly back to the original training data. The company releases open-sourced models and posts its code to repositories like GitHub for anyone to use. </p><p>Competitors like Google and OpenAI have <a href=\"https://venturebeat.com/ai/googles-gemini-transparency-cut-leaves-enterprise-developers-debugging-blind\"><u>faced criticism from developers</u></a> over moves that hid raw reasoning tokens and chose to summarize reasoning, claiming that they now resort to “debugging blind” without transparency. </p><p>Ai2 pretrained Olmo 3 on the six-trillion-token open source dataset, Dolma 3. The dataset encompasses web data, scientific literature and code. Smith said they optimized Olmo 3 for code, compared to the focus on math for Olmo 2. </p><h2>How it stacks up</h2><p>Ai2 claims that the Olmo 3 family of models represents a significant leap for truly open-source models, at least for open-source LLMs developed outside China. The base Olmo 3 model trained “with roughly 2.5x greater compute efficiency as measured by GPU-hours per token,” meaning it consumed less energy during pre-training and costs less.</p><p>The company said the Olmo 3 models outperformed other open models, such as Marin from Stanford, LLM360’s K2, and Apertus, though Ai2 did not provide figures for the benchmark testing. </p><p>“Of note, Olmo 3-Think (32B) is the strongest fully open reasoning model, narrowing the gap to the best open-weight models of similar scale, such as the Qwen 3-32B-Thinking series of models across our suite of reasoning benchmarks, all while being trained on 6x fewer tokens,” Ai2 said in a press release. </p><p>The company added that Olmo 3-Instruct performed better than Qwen 2.5, Gemma 3 and Llama 3.1.</p><p>\n\n</p><p> </p><p>\n</p>",
      "source": "AI | VentureBeat",
      "source_url": "https://venturebeat.com/category/ai/feed/",
      "author": "",
      "tags": [
        "AI"
      ],
      "relevance_score": 75,
      "category": "Frameworks & Tools"
    },
    {
      "title": "OpenAI Debuts GPT-5.1-Codex-Max, a Long-Horizon Agentic Coding Model With Compaction for Multi-Window Workflows",
      "url": "https://www.marktechpost.com/2025/11/19/openai-debuts-gpt-5-1-codex-max-a-long-horizon-agentic-coding-model-with-compaction-for-multi-window-workflows/",
      "published": "2025-11-20T08:55:27+00:00",
      "date_verified": true,
      "description": "<p>OpenAI has introduced GPT-5.1-Codex-Max, a frontier agentic coding model designed for long running software engineering tasks that span millions of tokens and multi hour sessions. It is available today inside Codex in the CLI, IDE extension, cloud integration and code review surfaces, with API access planned soon. What GPT-5.1-Codex-Max is optimised for? GPT-5.1-Codex-Max is built [&#8230;]</p>\n<p>The post <a href=\"https://www.marktechpost.com/2025/11/19/openai-debuts-gpt-5-1-codex-max-a-long-horizon-agentic-coding-model-with-compaction-for-multi-window-workflows/\">OpenAI Debuts GPT-5.1-Codex-Max, a Long-Horizon Agentic Coding Model With Compaction for Multi-Window Workflows</a> appeared first on <a href=\"https://www.marktechpost.com\">MarkTechPost</a>.</p>",
      "source": "MarkTechPost",
      "source_url": "https://www.marktechpost.com/feed/",
      "author": "Michal Sutter",
      "tags": [
        "Agentic AI",
        "AI Agents",
        "Editors Pick",
        "New Releases"
      ],
      "relevance_score": 75,
      "category": "Frameworks & Tools"
    },
    {
      "title": "Building more with GPT-5.1-Codex-Max",
      "url": "https://simonwillison.net/2025/Nov/19/gpt-51-codex-max/#atom-everything",
      "published": "2025-11-20T04:15:10+00:00",
      "date_verified": true,
      "description": "<p><strong><a href=\"https://openai.com/index/gpt-5-1-codex-max/\">Building more with GPT-5.1-Codex-Max</a></strong></p>\nHot on the heels of yesterday's <a href=\"https://simonwillison.net/2025/Nov/18/gemini-3/\">Gemini 3 Pro release</a> comes a new model from OpenAI called GPT-5.1-Codex-Max.</p>\n<p>(Remember when GPT-5 was meant to bring in a new era of less confusing model names? That didn't last!)</p>\n<p>It's currently only available through their <a href=\"https://developers.openai.com/codex/cli/\">Codex CLI coding agent</a>, where it's the new default model:</p>\n<blockquote>\n<p>Starting today, GPT‑5.1-Codex-Max will replace GPT‑5.1-Codex as the default model in Codex surfaces. Unlike GPT‑5.1, which is a general-purpose model, we recommend using GPT‑5.1-Codex-Max and the Codex family of models only for agentic coding tasks in Codex or Codex-like environments.</p>\n</blockquote>\n<p>It's not available via the API yet but should be shortly.</p>\n<p>The timing of this release is interesting given that Gemini 3 Pro appears to have <a href=\"https://simonwillison.net/2025/Nov/18/gemini-3/#benchmarks\">aced almost all of the benchmarks</a> just yesterday. It's reminiscent of the period in 2024 when OpenAI consistently made big announcements that happened to coincide with Gemini releases.</p>\n<p>OpenAI's self-reported <a href=\"https://openai.com/index/introducing-swe-bench-verified/\">SWE-Bench Verified</a> score is particularly notable: 76.5% for thinking level \"high\" and 77.9% for the new \"xhigh\". That was the one benchmark where Gemini 3 Pro was out-performed by Claude Sonnet 4.5 - Gemini 3 Pro got 76.2% and Sonnet 4.5 got 77.2%. OpenAI now have the highest scoring model there by a full .7 of a percentage point!</p>\n<p>They also report a score of 58.1% on <a href=\"https://www.tbench.ai/leaderboard/terminal-bench/2.0\">Terminal Bench 2.0</a>, beating Gemini 3 Pro's 54.2% (and Sonnet 4.5's 42.8%.)</p>\n<p>The most intriguing part of this announcement concerns the model's approach to long context problems:</p>\n<blockquote>\n<p>GPT‑5.1-Codex-Max is built for long-running, detailed work. It’s our first model natively trained to operate across multiple context windows through a process called <em>compaction</em>, coherently working over millions of tokens in a single task. [...]</p>\n<p>Compaction enables GPT‑5.1-Codex-Max to complete tasks that would have previously failed due to context-window limits, such as complex refactors and long-running agent loops by pruning its history while preserving the most important context over long horizons. In Codex applications, GPT‑5.1-Codex-Max automatically compacts its session when it approaches its context window limit, giving it a fresh context window. It repeats this process until the task is completed.</p>\n</blockquote>\n<p>There's a lot of confusion <a href=\"https://news.ycombinator.com/item?id=45982649\">on Hacker News</a> about what this actually means. Claude Code already does a version of compaction, automatically summarizing previous turns when the context runs out. Does this just mean that Codex-Max is better at that process?</p>\n<p>I had it draw me a couple of pelicans by typing \"Generate an SVG of a pelican riding a bicycle\" directly into the Codex CLI tool. Here's thinking level medium:</p>\n<p><img alt=\"A flat-style illustration shows a white, round-bodied bird with an orange beak pedaling a red-framed bicycle with thin black wheels along a sandy beach, with a calm blue ocean and clear sky in the background.\" src=\"https://static.simonwillison.net/static/2025/codex-max-medium.jpg\" /></p>\n<p>And here's thinking level \"xhigh\":</p>\n<p><img alt=\"A plump white bird with an orange beak and small black eyes crouches low on a blue bicycle with oversized dark wheels, shown racing forward with motion lines against a soft gradient blue sky.\" src=\"https://static.simonwillison.net/static/2025/codex-max-xhigh.jpg\" /></p>\n<p>I also tried xhigh on the my <a href=\"https://simonwillison.net/2025/Nov/18/gemini-3/#and-a-new-pelican-benchmark\">longer pelican test prompt</a>, which came out like this:</p>\n<p><img alt=\"A stylized dark gray bird with layered wings, a yellow head crest, and a long brown beak leans forward in a racing pose on a black-framed bicycle, riding across a glossy blue surface under a pale sky.\" src=\"https://static.simonwillison.net/static/2025/codex-breeding-max-xhigh.jpg\" /></p>\n<p>Also today: <a href=\"https://x.com/openai/status/1991266192905179613\">GPT-5.1 Pro is rolling out today to all Pro users</a>. According to the <a href=\"https://help.openai.com/en/articles/6825453-chatgpt-release-notes\">ChatGPT release notes</a>:</p>\n<blockquote>\n<p>GPT-5.1 Pro is rolling out today for all ChatGPT Pro users and is available in the model picker. GPT-5 Pro will remain available as a legacy model for 90 days before being retired.</p>\n</blockquote>\n<p>That's a pretty fast deprecation cycle for the GPT-5 Pro model that was released just three months ago.\n\n    <p><small></small>Via <a href=\"https://news.ycombinator.com/item?id=45982649\">Hacker News</a></small></p>\n\n\n    <p>Tags: <a href=\"https://simonwillison.net/tags/ai\">ai</a>, <a href=\"https://simonwillison.net/tags/openai\">openai</a>, <a href=\"https://simonwillison.net/tags/generative-ai\">generative-ai</a>, <a href=\"https://simonwillison.net/tags/llms\">llms</a>, <a href=\"https://simonwillison.net/tags/evals\">evals</a>, <a href=\"https://simonwillison.net/tags/pelican-riding-a-bicycle\">pelican-riding-a-bicycle</a>, <a href=\"https://simonwillison.net/tags/llm-release\">llm-release</a>, <a href=\"https://simonwillison.net/tags/gpt-5\">gpt-5</a>, <a href=\"https://simonwillison.net/tags/codex-cli\">codex-cli</a></p>",
      "source": "Simon Willison's Weblog",
      "source_url": "https://simonwillison.net/atom/everything/",
      "author": "",
      "tags": [
        "ai",
        "openai",
        "generative-ai",
        "llms",
        "evals",
        "pelican-riding-a-bicycle",
        "llm-release",
        "gpt-5",
        "codex-cli"
      ],
      "relevance_score": 75,
      "category": "Frameworks & Tools"
    },
    {
      "title": "Google Antigravity Makes the IDE a Control Plane for Agentic Coding",
      "url": "https://www.marktechpost.com/2025/11/19/google-antigravity-makes-the-ide-a-control-plane-for-agentic-coding/",
      "published": "2025-11-20T04:02:27+00:00",
      "date_verified": true,
      "description": "<p>Google has introduced Antigravity as an agentic development platform that sits on top of Gemini 3. It is not only an autocomplete layer, it is an IDE where agents plan, execute, and explain complex software tasks across editor, terminal, and browser surfaces. Antigravity was launched on November 18, 2025, alongside Gemini 3 as part of [&#8230;]</p>\n<p>The post <a href=\"https://www.marktechpost.com/2025/11/19/google-antigravity-makes-the-ide-a-control-plane-for-agentic-coding/\">Google Antigravity Makes the IDE a Control Plane for Agentic Coding</a> appeared first on <a href=\"https://www.marktechpost.com\">MarkTechPost</a>.</p>",
      "source": "MarkTechPost",
      "source_url": "https://www.marktechpost.com/feed/",
      "author": "Michal Sutter",
      "tags": [
        "Agentic AI",
        "AI Agents",
        "Editors Pick",
        "New Releases"
      ],
      "relevance_score": 75,
      "category": "Frameworks & Tools"
    },
    {
      "title": "How I automate my Substack newsletter with content from my blog",
      "url": "https://simonwillison.net/2025/Nov/19/how-i-automate-my-substack-newsletter/#atom-everything",
      "published": "2025-11-20T03:00:34+00:00",
      "date_verified": true,
      "description": "<p>I sent out <a href=\"https://simonw.substack.com/p/trying-out-gemini-3-pro-with-audio\">my weekly-ish Substack newsletter</a> this morning and took the opportunity to record <a href=\"https://www.youtube.com/watch?v=BoPZltKDM-s\">a YouTube video</a> demonstrating my process and describing the different components that make it work. There's a <em>lot</em> of digital duct tape involved, taking the content from Django+Heroku+PostgreSQL to GitHub Actions to SQLite+Datasette+Fly.io to JavaScript+Observable and finally to Substack.</p>\n\n<p> </p>\n\n<p>The core process is the same as I described <a href=\"https://simonwillison.net/2023/Apr/4/substack-observable/\">back in 2023</a>. I have an Observable notebook called <a href=\"https://observablehq.com/@simonw/blog-to-newsletter\">blog-to-newsletter</a> which fetches content from my blog's database, filters out anything that has been in the newsletter before, formats what's left as HTML and offers a big \"Copy rich text newsletter to clipboard\" button.</p>\n<p><img alt=\"Screenshot of the interface. An item in a list says 9080: Trying out Gemini 3 Pro with audio transcription and a new pelican benchmark. A huge button reads Copy rich text newsletter to clipboard - below is a smaller button that says Copy just the links/quotes/TILs. A Last X days slider is set to 2. There are checkboxes for SKip content sent in prior newsletters and only include post content prior to the cutoff comment.\" src=\"https://static.simonwillison.net/static/2025/copy-to-newsletter.jpg\" /></p>\n<p>I click that button, paste the result into the Substack editor, tweak a few things and hit send. The whole process usually takes just a few minutes.</p>\n<p>I make very minor edits:</p>\n<ul>\n<li>I set the title and the subheading for the newsletter. This is often a direct copy of the title of the featured blog post.</li>\n<li>Substack turns YouTube URLs into embeds, which often isn't what I want - especially if I have a YouTube URL inside a code example.</li>\n<li>Blocks of preformatted text often have an extra blank line at the end, which I remove.</li>\n<li>Occasionally I'll make a content edit - removing a piece of content that doesn't fit the newsletter, or fixing a time reference like \"yesterday\" that doesn't make sense any more.</li>\n<li>I pick the featured image for the newsletter and add some tags.</li>\n</ul>\n<p>That's the whole process!</p>\n<h4 id=\"the-observable-notebook\">The Observable notebook</h4>\n<p>The most important cell in the Observable notebook is this one:</p>\n<div class=\"highlight highlight-source-js\"><pre><span class=\"pl-s1\">raw_content</span> <span class=\"pl-c1\">=</span> <span class=\"pl-kos\">{</span>\n  <span class=\"pl-c1\">return</span> <span class=\"pl-s1\">await</span> <span class=\"pl-kos\">(</span>\n    <span class=\"pl-k\">await</span> <span class=\"pl-en\">fetch</span><span class=\"pl-kos\">(</span>\n      <span class=\"pl-s\">`https://datasette.simonwillison.net/simonwillisonblog.json?sql=<span class=\"pl-s1\"><span class=\"pl-kos\">${</span><span class=\"pl-en\">encodeURIComponent</span><span class=\"pl-kos\">(</span></span></span>\n<span class=\"pl-s\"><span class=\"pl-s1\">        <span class=\"pl-s1\">sql</span></span></span>\n<span class=\"pl-s\"><span class=\"pl-s1\">      <span class=\"pl-kos\">)</span><span class=\"pl-kos\">}</span></span>&amp;_shape=array&amp;numdays=<span class=\"pl-s1\"><span class=\"pl-kos\">${</span><span class=\"pl-s1\">numDays</span><span class=\"pl-kos\">}</span></span>`</span>\n    <span class=\"pl-kos\">)</span>\n  <span class=\"pl-kos\">)</span><span class=\"pl-kos\">.</span><span class=\"pl-en\">json</span><span class=\"pl-kos\">(</span><span class=\"pl-kos\">)</span><span class=\"pl-kos\">;</span>\n<span class=\"pl-kos\">}</span></pre></div>\n<p>This uses the JavaScript <code>fetch()</code> function to pull data from my blog's Datasette instance, using a very complex SQL query that is composed elsewhere in the notebook.</p>\n<p>Here's a link to <a href=\"https://datasette.simonwillison.net/simonwillisonblog?sql=with+content+as+%28%0D%0A++select%0D%0A++++id%2C%0D%0A++++%27entry%27+as+type%2C%0D%0A++++title%2C%0D%0A++++created%2C%0D%0A++++slug%2C%0D%0A++++%27%3Ch3%3E%3Ca+href%3D%22%27+%7C%7C+%27https%3A%2F%2Fsimonwillison.net%2F%27+%7C%7C+strftime%28%27%25Y%2F%27%2C+created%29%0D%0A++++++%7C%7C+substr%28%27JanFebMarAprMayJunJulAugSepOctNovDec%27%2C+%28strftime%28%27%25m%27%2C+created%29+-+1%29+*+3+%2B+1%2C+3%29+%0D%0A++++++%7C%7C+%27%2F%27+%7C%7C+cast%28strftime%28%27%25d%27%2C+created%29+as+integer%29+%7C%7C+%27%2F%27+%7C%7C+slug+%7C%7C+%27%2F%27+%7C%7C+%27%22%3E%27+%0D%0A++++++%7C%7C+title+%7C%7C+%27%3C%2Fa%3E+-+%27+%7C%7C+date%28created%29+%7C%7C+%27%3C%2Fh3%3E%27+%7C%7C+body%0D%0A++++++as+html%2C%0D%0A++++%27null%27+as+json%2C%0D%0A++++%27%27+as+external_url%0D%0A++from+blog_entry%0D%0A++union+all%0D%0A++select%0D%0A++++id%2C%0D%0A++++%27blogmark%27+as+type%2C%0D%0A++++link_title%2C%0D%0A++++created%2C%0D%0A++++slug%2C%0D%0A++++%27%3Cp%3E%3Cstrong%3ELink%3C%2Fstrong%3E+%27+%7C%7C+date%28created%29+%7C%7C+%27+%3Ca+href%3D%22%27%7C%7C+link_url+%7C%7C+%27%22%3E%27%0D%0A++++++%7C%7C+link_title+%7C%7C+%27%3C%2Fa%3E%3A%3C%2Fp%3E%3Cp%3E%27+%7C%7C+%27+%27+%7C%7C+replace%28commentary%2C+%27%0D%0A%27%2C+%27%3Cbr%3E%27%29+%7C%7C+%27%3C%2Fp%3E%27%0D%0A++++++as+html%2C%0D%0A++++json_object%28%0D%0A++++++%27created%27%2C+date%28created%29%2C%0D%0A++++++%27link_url%27%2C+link_url%2C%0D%0A++++++%27link_title%27%2C+link_title%2C%0D%0A++++++%27commentary%27%2C+commentary%2C%0D%0A++++++%27use_markdown%27%2C+use_markdown%0D%0A++++%29+as+json%2C%0D%0A++link_url+as+external_url%0D%0A++from+blog_blogmark%0D%0A++union+all%0D%0A++select%0D%0A++++id%2C%0D%0A++++%27quotation%27+as+type%2C%0D%0A++++source%2C%0D%0A++++created%2C%0D%0A++++slug%2C%0D%0A++++%27%3Cstrong%3Equote%3C%2Fstrong%3E+%27+%7C%7C+date%28created%29+%7C%7C%0D%0A++++%27%3Cblockquote%3E%3Cp%3E%3Cem%3E%27+%7C%7C%0D%0A++++replace%28quotation%2C+%27%0D%0A%27%2C+%27%3Cbr%3E%27%29+%7C%7C+%0D%0A++++%27%3C%2Fem%3E%3C%2Fp%3E%3C%2Fblockquote%3E%3Cp%3E%3Ca+href%3D%22%27+%7C%7C%0D%0A++++coalesce%28source_url%2C+%27%23%27%29+%7C%7C+%27%22%3E%27+%7C%7C+source+%7C%7C+%27%3C%2Fa%3E%27+%7C%7C%0D%0A++++case+%0D%0A++++++++when+nullif%28trim%28context%29%2C+%27%27%29+is+not+null+%0D%0A++++++++then+%27%2C+%27+%7C%7C+context+%0D%0A++++++++else+%27%27+%0D%0A++++end+%7C%7C%0D%0A++++%27%3C%2Fp%3E%27+as+html%2C%0D%0A++++%27null%27+as+json%2C%0D%0A++++source_url+as+external_url%0D%0A++from+blog_quotation%0D%0A++union+all%0D%0A++select%0D%0A++++id%2C%0D%0A++++%27note%27+as+type%2C%0D%0A++++case%0D%0A++++++when+title+is+not+null+and+title+%3C%3E+%27%27+then+title%0D%0A++++++else+%27Note+on+%27+%7C%7C+date%28created%29%0D%0A++++end%2C%0D%0A++++created%2C%0D%0A++++slug%2C%0D%0A++++%27No+HTML%27%2C%0D%0A++++json_object%28%0D%0A++++++%27created%27%2C+date%28created%29%2C%0D%0A++++++%27link_url%27%2C+%27https%3A%2F%2Fsimonwillison.net%2F%27+%7C%7C+strftime%28%27%25Y%2F%27%2C+created%29%0D%0A++++++%7C%7C+substr%28%27JanFebMarAprMayJunJulAugSepOctNovDec%27%2C+%28strftime%28%27%25m%27%2C+created%29+-+1%29+*+3+%2B+1%2C+3%29+%0D%0A++++++%7C%7C+%27%2F%27+%7C%7C+cast%28strftime%28%27%25d%27%2C+created%29+as+integer%29+%7C%7C+%27%2F%27+%7C%7C+slug+%7C%7C+%27%2F%27%2C%0D%0A++++++%27link_title%27%2C+%27%27%2C%0D%0A++++++%27commentary%27%2C+body%2C%0D%0A++++++%27use_markdown%27%2C+1%0D%0A++++%29%2C%0D%0A++++%27%27+as+external_url%0D%0A++from+blog_note%0D%0A++union+all%0D%0A++select%0D%0A++++rowid%2C%0D%0A++++%27til%27+as+type%2C%0D%0A++++title%2C%0D%0A++++created%2C%0D%0A++++%27null%27+as+slug%2C%0D%0A++++%27%3Cp%3E%3Cstrong%3ETIL%3C%2Fstrong%3E+%27+%7C%7C+date%28created%29+%7C%7C+%27+%3Ca+href%3D%22%27%7C%7C+%27https%3A%2F%2Ftil.simonwillison.net%2F%27+%7C%7C+topic+%7C%7C+%27%2F%27+%7C%7C+slug+%7C%7C+%27%22%3E%27+%7C%7C+title+%7C%7C+%27%3C%2Fa%3E%3A%27+%7C%7C+%27+%27+%7C%7C+substr%28html%2C+1%2C+instr%28html%2C+%27%3C%2Fp%3E%27%29+-+1%29+%7C%7C+%27+%26%238230%3B%3C%2Fp%3E%27+as+html%2C%0D%0A++++%27null%27+as+json%2C%0D%0A++++%27https%3A%2F%2Ftil.simonwillison.net%2F%27+%7C%7C+topic+%7C%7C+%27%2F%27+%7C%7C+slug+as+external_url%0D%0A++from+til%0D%0A%29%2C%0D%0Acollected+as+%28%0D%0A++select%0D%0A++++id%2C%0D%0A++++type%2C%0D%0A++++title%2C%0D%0A++++case%0D%0A++++++when+type+%3D+%27til%27%0D%0A++++++then+external_url%0D%0A++++++else+%27https%3A%2F%2Fsimonwillison.net%2F%27+%7C%7C+strftime%28%27%25Y%2F%27%2C+created%29%0D%0A++++++%7C%7C+substr%28%27JanFebMarAprMayJunJulAugSepOctNovDec%27%2C+%28strftime%28%27%25m%27%2C+created%29+-+1%29+*+3+%2B+1%2C+3%29+%7C%7C+%0D%0A++++++%27%2F%27+%7C%7C+cast%28strftime%28%27%25d%27%2C+created%29+as+integer%29+%7C%7C+%27%2F%27+%7C%7C+slug+%7C%7C+%27%2F%27%0D%0A++++++end+as+url%2C%0D%0A++++created%2C%0D%0A++++html%2C%0D%0A++++json%2C%0D%0A++++external_url%2C%0D%0A++++case%0D%0A++++++when+type+%3D+%27entry%27+then+%28%0D%0A++++++++select+json_group_array%28tag%29%0D%0A++++++++from+blog_tag%0D%0A++++++++join+blog_entry_tags+on+blog_tag.id+%3D+blog_entry_tags.tag_id%0D%0A++++++++where+blog_entry_tags.entry_id+%3D+content.id%0D%0A++++++%29%0D%0A++++++when+type+%3D+%27blogmark%27+then+%28%0D%0A++++++++select+json_group_array%28tag%29%0D%0A++++++++from+blog_tag%0D%0A++++++++join+blog_blogmark_tags+on+blog_tag.id+%3D+blog_blogmark_tags.tag_id%0D%0A++++++++where+blog_blogmark_tags.blogmark_id+%3D+content.id%0D%0A++++++%29%0D%0A++++++when+type+%3D+%27quotation%27+then+%28%0D%0A++++++++select+json_group_array%28tag%29%0D%0A++++++++from+blog_tag%0D%0A++++++++join+blog_quotation_tags+on+blog_tag.id+%3D+blog_quotation_tags.tag_id%0D%0A++++++++where+blog_quotation_tags.quotation_id+%3D+content.id%0D%0A++++++%29%0D%0A++++++else+%27%5B%5D%27%0D%0A++++end+as+tags%0D%0A++from+content%0D%0A++where+created+%3E%3D+date%28%27now%27%2C+%27-%27+%7C%7C+%3Anumdays+%7C%7C+%27+days%27%29+++%0D%0A++order+by+created+desc%0D%0A%29%0D%0Aselect+id%2C+type%2C+title%2C+url%2C+created%2C+html%2C+json%2C+external_url%2C+tags%0D%0Afrom+collected+%0D%0Aorder+by+%0D%0A++case+type+%0D%0A++++when+%27entry%27+then+0+%0D%0A++++else+1+%0D%0A++end%2C%0D%0A++case+type+%0D%0A++++when+%27entry%27+then+created+%0D%0A++++else+-strftime%28%27%25s%27%2C+created%29+%0D%0A++end+desc%3B&amp;numdays=7\">see and execute that query</a> directly in Datasette. It's 143 lines of convoluted SQL that assembles most of the HTML for the newsletter using SQLite string concatenation! An illustrative snippet:</p>\n<div class=\"highlight highlight-source-sql\"><pre>with content <span class=\"pl-k\">as</span> (\n  <span class=\"pl-k\">select</span>\n    id,\n    <span class=\"pl-s\"><span class=\"pl-pds\">'</span>entry<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">as</span> type,\n    title,\n    created,\n    slug,\n    <span class=\"pl-s\"><span class=\"pl-pds\">'</span>&lt;h3&gt;&lt;a href=\"<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">||</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>https://simonwillison.net/<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">||</span> strftime(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>%Y/<span class=\"pl-pds\">'</span></span>, created)\n      <span class=\"pl-k\">||</span> substr(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>JanFebMarAprMayJunJulAugSepOctNovDec<span class=\"pl-pds\">'</span></span>, (strftime(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>%m<span class=\"pl-pds\">'</span></span>, created) <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1</span>) <span class=\"pl-k\">*</span> <span class=\"pl-c1\">3</span> <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>) \n      <span class=\"pl-k\">||</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>/<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">||</span> cast(strftime(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>%d<span class=\"pl-pds\">'</span></span>, created) <span class=\"pl-k\">as</span> <span class=\"pl-k\">integer</span>) <span class=\"pl-k\">||</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>/<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">||</span> slug <span class=\"pl-k\">||</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>/<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">||</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>\"&gt;<span class=\"pl-pds\">'</span></span> \n      <span class=\"pl-k\">||</span> title <span class=\"pl-k\">||</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>&lt;/a&gt; - <span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">||</span> <span class=\"pl-k\">date</span>(created) <span class=\"pl-k\">||</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>&lt;/h3&gt;<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">||</span> body\n      <span class=\"pl-k\">as</span> html,\n    <span class=\"pl-s\"><span class=\"pl-pds\">'</span>null<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">as</span> json,\n    <span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">as</span> external_url\n  <span class=\"pl-k\">from</span> blog_entry\n  <span class=\"pl-k\">union all</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> ...</span></pre></div>\n<p>My blog's URLs look like <code>/2025/Nov/18/gemini-3/</code> - this SQL constructs that three letter month abbreviation from the month number using a substring operation.</p>\n<p>This is a <em>terrible</em> way to assemble HTML, but I've stuck with it because it amuses me.</p>\n<p>The rest of the Observable notebook takes that data, filters out anything that links to content mentioned in the previous newsletters and composes it into a block of HTML that can be copied using that big button.</p>\n<p>Here's the recipe it uses to turn HTML into rich text content on a clipboard suitable for Substack. I can't remember how I figured this out but it's very effective:</p>\n<div class=\"highlight highlight-source-js\"><pre><span class=\"pl-v\">Object</span><span class=\"pl-kos\">.</span><span class=\"pl-en\">assign</span><span class=\"pl-kos\">(</span>\n  <span class=\"pl-en\">html</span><span class=\"pl-s\">`<span class=\"pl-kos\">&lt;</span><span class=\"pl-ent\">button</span> <span class=\"pl-c1\">style</span>=\"<span class=\"pl-s\">font-size: 1.4em; padding: 0.3em 1em; font-weight: bold;</span>\"<span class=\"pl-kos\">&gt;</span>Copy rich text newsletter to clipboard`</span><span class=\"pl-kos\">,</span>\n  <span class=\"pl-kos\">{</span>\n    <span class=\"pl-en\">onclick</span>: <span class=\"pl-kos\">(</span><span class=\"pl-kos\">)</span> <span class=\"pl-c1\">=&gt;</span> <span class=\"pl-kos\">{</span>\n      <span class=\"pl-k\">const</span> <span class=\"pl-s1\">htmlContent</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">newsletterHTML</span><span class=\"pl-kos\">;</span>\n      <span class=\"pl-c\">// Create a temporary element to hold the HTML content</span>\n      <span class=\"pl-k\">const</span> <span class=\"pl-s1\">tempElement</span> <span class=\"pl-c1\">=</span> <span class=\"pl-smi\">document</span><span class=\"pl-kos\">.</span><span class=\"pl-en\">createElement</span><span class=\"pl-kos\">(</span><span class=\"pl-s\">\"div\"</span><span class=\"pl-kos\">)</span><span class=\"pl-kos\">;</span>\n      <span class=\"pl-s1\">tempElement</span><span class=\"pl-kos\">.</span><span class=\"pl-c1\">innerHTML</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">htmlContent</span><span class=\"pl-kos\">;</span>\n      <span class=\"pl-smi\">document</span><span class=\"pl-kos\">.</span><span class=\"pl-c1\">body</span><span class=\"pl-kos\">.</span><span class=\"pl-en\">appendChild</span><span class=\"pl-kos\">(</span><span class=\"pl-s1\">tempElement</span><span class=\"pl-kos\">)</span><span class=\"pl-kos\">;</span>\n      <span class=\"pl-c\">// Select the HTML content</span>\n      <span class=\"pl-k\">const</span> <span class=\"pl-s1\">range</span> <span class=\"pl-c1\">=</span> <span class=\"pl-smi\">document</span><span class=\"pl-kos\">.</span><span class=\"pl-en\">createRange</span><span class=\"pl-kos\">(</span><span class=\"pl-kos\">)</span><span class=\"pl-kos\">;</span>\n      <span class=\"pl-s1\">range</span><span class=\"pl-kos\">.</span><span class=\"pl-en\">selectNode</span><span class=\"pl-kos\">(</span><span class=\"pl-s1\">tempElement</span><span class=\"pl-kos\">)</span><span class=\"pl-kos\">;</span>\n      <span class=\"pl-c\">// Copy the selected HTML content to the clipboard</span>\n      <span class=\"pl-k\">const</span> <span class=\"pl-s1\">selection</span> <span class=\"pl-c1\">=</span> <span class=\"pl-smi\">window</span><span class=\"pl-kos\">.</span><span class=\"pl-en\">getSelection</span><span class=\"pl-kos\">(</span><span class=\"pl-kos\">)</span><span class=\"pl-kos\">;</span>\n      <span class=\"pl-s1\">selection</span><span class=\"pl-kos\">.</span><span class=\"pl-en\">removeAllRanges</span><span class=\"pl-kos\">(</span><span class=\"pl-kos\">)</span><span class=\"pl-kos\">;</span>\n      <span class=\"pl-s1\">selection</span><span class=\"pl-kos\">.</span><span class=\"pl-en\">addRange</span><span class=\"pl-kos\">(</span><span class=\"pl-s1\">range</span><span class=\"pl-kos\">)</span><span class=\"pl-kos\">;</span>\n      <span class=\"pl-smi\">document</span><span class=\"pl-kos\">.</span><span class=\"pl-en\">execCommand</span><span class=\"pl-kos\">(</span><span class=\"pl-s\">\"copy\"</span><span class=\"pl-kos\">)</span><span class=\"pl-kos\">;</span>\n      <span class=\"pl-s1\">selection</span><span class=\"pl-kos\">.</span><span class=\"pl-en\">removeAllRanges</span><span class=\"pl-kos\">(</span><span class=\"pl-kos\">)</span><span class=\"pl-kos\">;</span>\n      <span class=\"pl-smi\">document</span><span class=\"pl-kos\">.</span><span class=\"pl-c1\">body</span><span class=\"pl-kos\">.</span><span class=\"pl-en\">removeChild</span><span class=\"pl-kos\">(</span><span class=\"pl-s1\">tempElement</span><span class=\"pl-kos\">)</span><span class=\"pl-kos\">;</span>\n    <span class=\"pl-kos\">}</span>\n  <span class=\"pl-kos\">}</span>\n<span class=\"pl-kos\">)</span></pre></div>\n<h4 id=\"from-django-postgresql-to-datasette-sqlite\">From Django+Postgresql to Datasette+SQLite</h4>\n<p>My blog itself is a Django application hosted on Heroku, with data stored in Heroku PostgreSQL. Here's <a href=\"https://github.com/simonw/simonwillisonblog\">the source code for that Django application</a>. I use the Django admin as my CMS.</p>\n<p><a href=\"https://datasette.io/\">Datasette</a> provides a JSON API over a SQLite database... which means something needs to convert that PostgreSQL database into a SQLite database that Datasette can use.</p>\n<p>My system for doing that lives in the <a href=\"https://github.com/simonw/simonwillisonblog-backup\">simonw/simonwillisonblog-backup</a> GitHub repository. It uses GitHub Actions on a schedule that executes every two hours, fetching the latest data from PostgreSQL and converting that to SQLite.</p>\n<p>My <a href=\"https://github.com/simonw/db-to-sqlite\">db-to-sqlite</a> tool is responsible for that conversion. I call it <a href=\"https://github.com/simonw/simonwillisonblog-backup/blob/dc5b9df272134ce051a5280b4de6d4daa9b2a9fc/.github/workflows/backup.yml#L44-L62\">like this</a>:</p>\n<div class=\"highlight highlight-source-shell\"><pre>db-to-sqlite \\\n  <span class=\"pl-s\"><span class=\"pl-pds\">$(</span>heroku config:get DATABASE_URL -a simonwillisonblog <span class=\"pl-k\">|</span> sed s/postgres:/postgresql+psycopg2:/<span class=\"pl-pds\">)</span></span> \\\n  simonwillisonblog.db \\\n  --table auth_permission \\\n  --table auth_user \\\n  --table blog_blogmark \\\n  --table blog_blogmark_tags \\\n  --table blog_entry \\\n  --table blog_entry_tags \\\n  --table blog_quotation \\\n  --table blog_quotation_tags \\\n  --table blog_note \\\n  --table blog_note_tags \\\n  --table blog_tag \\\n  --table blog_previoustagname \\\n  --table blog_series \\\n  --table django_content_type \\\n  --table redirects_redirect</pre></div>\n<p>That <code>heroku config:get DATABASE_URL</code> command uses Heroku credentials in an environment variable to fetch the database connection URL for my blog's PostgreSQL database (and fixes a small difference in the URL scheme).</p>\n<p><code>db-to-sqlite</code> can then export that data and write it to a SQLite database file called <code>simonwillisonblog.db</code>.</p>\n<p>The <code>--table</code> options specify the tables that should be included in the export.</p>\n<p>The repository does more than just that conversion: it also exports the resulting data to JSON files that live in the repository, which gives me a <a href=\"https://github.com/simonw/simonwillisonblog-backup/commits/main/simonwillisonblog\">commit history</a> of changes I make to my content. This is a cheap way to get a revision history of my blog content without having to mess around with detailed history tracking inside the Django application itself.</p>\n<p>At the <a href=\"https://github.com/simonw/simonwillisonblog-backup/blob/dc5b9df272134ce051a5280b4de6d4daa9b2a9fc/.github/workflows/backup.yml#L200-L204\">end of my GitHub Actions workflow</a> is this code that publishes the resulting database to Datasette running on <a href=\"https://fly.io/\">Fly.io</a> using the <a href=\"https://datasette.io/plugins/datasette-publish-fly\">datasette publish fly</a> plugin:</p>\n<div class=\"highlight highlight-source-shell\"><pre>datasette publish fly simonwillisonblog.db \\\n  -m metadata.yml \\\n  --app simonwillisonblog-backup \\\n  --branch 1.0a2 \\\n  --extra-options <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>--setting sql_time_limit_ms 15000 --setting truncate_cells_html 10000 --setting allow_facet off<span class=\"pl-pds\">\"</span></span> \\\n  --install datasette-block-robots \\\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> ... more plugins</span></pre></div>\n<p>As you can see, there are a lot of moving parts! Surprisingly it all mostly just works - I rarely have to intervene in the process, and the cost of those different components is pleasantly low.</p>\n    \n        <p>Tags: <a href=\"https://simonwillison.net/tags/blogging\">blogging</a>, <a href=\"https://simonwillison.net/tags/django\">django</a>, <a href=\"https://simonwillison.net/tags/javascript\">javascript</a>, <a href=\"https://simonwillison.net/tags/postgresql\">postgresql</a>, <a href=\"https://simonwillison.net/tags/sql\">sql</a>, <a href=\"https://simonwillison.net/tags/sqlite\">sqlite</a>, <a href=\"https://simonwillison.net/tags/youtube\">youtube</a>, <a href=\"https://simonwillison.net/tags/heroku\">heroku</a>, <a href=\"https://simonwillison.net/tags/datasette\">datasette</a>, <a href=\"https://simonwillison.net/tags/observable\">observable</a>, <a href=\"https://simonwillison.net/tags/github-actions\">github-actions</a>, <a href=\"https://simonwillison.net/tags/fly\">fly</a>, <a href=\"https://simonwillison.net/tags/newsletter\">newsletter</a></p>",
      "source": "Simon Willison's Weblog",
      "source_url": "https://simonwillison.net/atom/everything/",
      "author": "",
      "tags": [
        "blogging",
        "django",
        "javascript",
        "postgresql",
        "sql",
        "sqlite",
        "youtube",
        "heroku",
        "datasette",
        "observable",
        "github-actions",
        "fly",
        "newsletter"
      ],
      "relevance_score": 75,
      "category": "Frameworks & Tools"
    }
  ]
}